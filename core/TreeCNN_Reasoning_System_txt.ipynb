{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1:"
      ],
      "metadata": {
        "id": "WijvBVIsv_GZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yPQd_7WJzIAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny83u4yxOjpL",
        "outputId": "9213ce74-144d-4c10-c8f4-79c2d493cfc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/TLiteComponents.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/TLiteComponents.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\"\"\"InputProjector: This is a simple linear layer that transforms an input vector from one dimension to another.\n",
        "It's useful for ensuring that different parts of a model expect and receive feature vectors of a consistent size.\"\"\"\n",
        "\n",
        "class InputProjector(nn.Module):\n",
        "    def __init__(self, input_dim, target_dim):\n",
        "        super().__init__()\n",
        "        self.project = nn.Linear(input_dim, target_dim).to(input_dim.device if isinstance(input_dim, torch.Tensor) else 'cpu')\n",
        "        logger.info(f\"Initialized InputProjector: {input_dim} -> {target_dim}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.project(x)\n",
        "\"\"\"\n",
        "patch_model_input: A utility function that can dynamically add an InputProjector to an existing model.\n",
        "If a model expects an input of a certain dimension but receives a different one, this function can 'patch' the model's forward method to include the projection automatically.\n",
        "\n",
        "\"\"\"\n",
        "def patch_model_input(model, input_vector, expected_dim):\n",
        "    actual_dim = input_vector.shape[-1]\n",
        "    if actual_dim != expected_dim:\n",
        "        if hasattr(model, 'input_projector'):\n",
        "            logger.warning(f\"Model already has input_projector, skipping patch\")\n",
        "            return\n",
        "        logger.info(f\"Auto-patching model input: {actual_dim} -> {expected_dim}\")\n",
        "        projector = InputProjector(actual_dim, expected_dim).to(input_vector.device)\n",
        "        old_forward = model.forward\n",
        "\n",
        "        def new_forward(x):\n",
        "            x_proj = projector(x)\n",
        "            return old_forward(x_proj)\n",
        "\n",
        "        model.forward = new_forward\n",
        "        model.input_projector = projector\n",
        "\n",
        "class ShapeTagEmbedder(nn.Module):\n",
        "    def __init__(self, shape_vocab, dim, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.shape_vocab = shape_vocab\n",
        "        self.tag2idx = {tag: i for i, tag in enumerate(shape_vocab)}\n",
        "        self.embedding = nn.Embedding(len(shape_vocab), dim).to(device)\n",
        "        logger.info(f\"ShapeTagEmbedder initialized with {len(shape_vocab)} tags, dim={dim}\")\n",
        "\n",
        "    def forward(self, shape_tag: str):\n",
        "        idx = self.tag2idx.get(shape_tag, 0)\n",
        "        idx_tensor = torch.tensor([idx], dtype=torch.long, device=self.device)\n",
        "        return self.embedding(idx_tensor).squeeze(0)\n",
        "\n",
        "class TreeEncoderWithAttention(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int = 5, device: str = 'cpu'):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.device = device\n",
        "        self.projectors = {}\n",
        "        if dim % num_heads != 0:\n",
        "            num_heads = max(1, dim // 4)\n",
        "            logger.warning(f\"Adjusted num_heads to {num_heads} for embed_dim={dim}\")\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads).to(device)\n",
        "        self.norm = nn.LayerNorm(dim).to(device)\n",
        "        self.shape_embedder = ShapeTagEmbedder(\n",
        "            shape_vocab=['unknown', 'text', 'circle', 'line', 'arrow', 'box'],\n",
        "            dim=dim, device=device\n",
        "        )\n",
        "        logger.info(f\"Initialized TreeEncoderWithAttention with dim={dim}\")\n",
        "\n",
        "    def _ensure_dim(self, vec: torch.Tensor) -> torch.Tensor:\n",
        "        vec = vec.to(self.device)\n",
        "        if vec.shape[-1] == self.dim:\n",
        "            return vec\n",
        "        key = f\"{vec.shape[-1]}>{self.dim}\"\n",
        "        if key not in self.projectors:\n",
        "            self.projectors[key] = nn.Linear(vec.shape[-1], self.dim).to(self.device)\n",
        "            logger.info(f\"Auto-projecting leaf: {vec.shape[-1]} -> {self.dim}\")\n",
        "        return self.projectors[key](vec)\n",
        "\n",
        "    def encode(self, node, get_vector_fn):\n",
        "        if not hasattr(node, 'is_leaf') or not hasattr(node, 'get_vector'):\n",
        "            logger.error(f\"Invalid node type: {type(node)}\")\n",
        "            return torch.zeros(self.dim, device=self.device)\n",
        "        if node.is_leaf():\n",
        "            token_vec = get_vector_fn(node)\n",
        "            shape_tag = getattr(node, 'shape_tag', 'unknown')\n",
        "            shape_vec = self.shape_embedder(shape_tag)\n",
        "            if token_vec is None or not isinstance(token_vec, torch.Tensor):\n",
        "                logger.warning(f\"Leaf missing token vector, using shape vector: {shape_tag}\")\n",
        "                return shape_vec\n",
        "            combined = token_vec.to(self.device) + shape_vec\n",
        "            return self._ensure_dim(combined)\n",
        "\n",
        "        vectors = [self.encode(child, get_vector_fn) for child in node.children if child]\n",
        "        if not vectors:\n",
        "            return torch.zeros(self.dim, device=self.device)\n",
        "        stacked = torch.stack(vectors).unsqueeze(1)\n",
        "        attn_output, _ = self.attention(stacked, stacked, stacked)\n",
        "        return self.norm(attn_output.squeeze(1).mean(dim=0))\n",
        "\n",
        "    def forward(self, node, get_vector_fn=lambda n: n.get_vector()):\n",
        "        return self.encode(node, get_vector_fn)\n",
        "\n",
        "\"\"\"TLiteV5_ReasoningModule: This module appears to be a multi-layer neural network designed for reasoning.\n",
        " It uses a stack of LayerNorm, Linear, and GELU activation layers.\n",
        " It also incorporates an InputProjector to ensure its input is of the expected dimension, and a final Softplus activated linear layer,\n",
        " typically used for outputting positive scores or confidence values.\"\"\"\n",
        "\n",
        "class TLiteV5_ReasoningModule(nn.Module):\n",
        "    def __init__(self, dim=50, hidden_dim=128, depth=4, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.expected_dim = dim\n",
        "        self.device = device\n",
        "        self.projector = None\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.LayerNorm(dim),\n",
        "                nn.Linear(dim, hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(hidden_dim, dim)\n",
        "            ) for _ in range(depth)\n",
        "        ]).to(device)\n",
        "        self.final_norm = nn.LayerNorm(dim).to(device)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(dim, 1),\n",
        "            nn.Softplus()\n",
        "        ).to(device)\n",
        "        logger.info(f\"Initialized TLiteV5_ReasoningModule with dim={dim}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x is None:\n",
        "            logger.warning(\"Invalid input, returning zero\")\n",
        "            return torch.tensor(0.0, device=self.device)\n",
        "        x = x.to(self.device)\n",
        "        if x.shape[-1] != self.expected_dim:\n",
        "            if self.projector is None:\n",
        "                self.projector = InputProjector(x.shape[-1], self.expected_dim).to(self.device)\n",
        "            x = self.projector(x)\n",
        "        for layer in self.layers:\n",
        "            x = x + layer(x)\n",
        "        x = self.final_norm(x)\n",
        "        return self.head(x).squeeze(-1)\n",
        "\n",
        "\"\"\"TLiteExpert: This represents a single 'expert' network, a small multi-layer perceptron (MLP) with LayerNorm and GELU activations.\n",
        "These experts are commonly used in Mixture of Experts (MoE) architectures.\"\"\"\n",
        "\n",
        "class TLiteExpert(nn.Module):\n",
        "    def __init__(self, dim=50, hidden_dim=64, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim).to(device)\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim).to(device)\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim).to(device)\n",
        "        self.device = device\n",
        "        logger.info(f\"Initialized TLiteExpert with dim={dim}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x is None:\n",
        "            logger.warning(\"Invalid input, returning zero\")\n",
        "            return torch.zeros(self.dim, device=self.device)\n",
        "        x = x.to(self.device)\n",
        "        x = self.norm(x)\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\"\"\"TLiteRouter: This module is part of an MoE system. It determines which TLiteExpert models should process a given input.\n",
        "It takes an input vector, calculates scores for each expert, and then uses a softmax function to select the top_k experts and assign weights to their outputs.\"\"\"\n",
        "class TLiteRouter(nn.Module):\n",
        "    def __init__(self, dim=50, num_experts=8, top_k=2, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.gate = nn.Linear(dim, num_experts).to(device)\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        self.device = device\n",
        "        logger.info(f\"Initialized TLiteRouter with {num_experts} experts\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x is None:\n",
        "            logger.warning(\"Invalid input, returning zeros\")\n",
        "            return torch.zeros(self.num_experts, device=self.device), torch.zeros(self.top_k, device=self.device)\n",
        "        x = x.to(self.device)\n",
        "        scores = self.gate(x)\n",
        "        topk_scores, topk_indices = torch.topk(scores, self.top_k, dim=-1)\n",
        "        topk_weights = F.softmax(topk_scores, dim=-1)\n",
        "        return topk_indices, topk_weights\n",
        "\"\"\"TLiteV6: This combines the TLiteExpert and TLiteRouter to form a complete Mixture of Experts model.\n",
        " When data is passed to TLiteV6, the Router selects a few 'experts' to process the data, their outputs are combined based on the router's weights,\n",
        "  and then passed through a final head for the ultimate prediction.\"\"\"\n",
        "class TLiteV6(nn.Module):\n",
        "    def __init__(self, dim=50, hidden_dim=64, num_experts=8, top_k=2, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.experts = nn.ModuleList([TLiteExpert(dim, hidden_dim, device) for _ in range(num_experts)])\n",
        "        self.router = TLiteRouter(dim, num_experts, top_k, device)\n",
        "        self.final_head = nn.Sequential(\n",
        "            nn.Linear(dim, 1),\n",
        "            nn.Softplus()\n",
        "        ).to(device)\n",
        "        self.device = device\n",
        "        logger.info(f\"Initialized TLiteV6 with {num_experts} experts\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x is None:\n",
        "            logger.warning(\"Invalid input, returning zero\")\n",
        "            return torch.tensor(0.0, device=self.device)\n",
        "        x = x.to(self.device)\n",
        "        topk_indices, topk_weights = self.router(x)\n",
        "        out = torch.zeros_like(x, device=self.device)\n",
        "        for b in range(x.shape[0]):\n",
        "            for i, idx in enumerate(topk_indices[b]):\n",
        "                expert_out = self.experts[idx](x[b])\n",
        "                out[b] += topk_weights[b][i] * expert_out\n",
        "        return self.final_head(out).squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk_Y1OTW_eIk",
        "outputId": "a99d4821-b2ee-450c-b880-a02089ac2913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/smart_utils.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/smart_utils.py\n",
        "\n",
        "import re\n",
        "import logging\n",
        "import sympy as sp\n",
        "from word2number import w2n\n",
        "import torch\n",
        "from typing import Any\n",
        "\n",
        "\"\"\"Its main purpose is to convert various forms of answers—whether they're numbers, words representing numbers,\n",
        "or even mathematical expressions—into a single, standardized floating-point number.\n",
        "Here's how it works:\n",
        "Handles Missing or Direct Numeric Inputs: If the input is None, it returns 0.0.\n",
        "If it's already an integer or a float, it simply converts it to a float and returns it.\n",
        "Processes Text Inputs: For anything else, especially text, it goes through several steps:\n",
        "Cleaning: It first cleans the text by removing most punctuation and converting it to lowercase, making it easier to process.\n",
        "Direct Number Conversion: It tries to convert the cleaned text directly into a float (e.g., '123.45' becomes 123.45).\n",
        "Word-to-Number Conversion: If that fails, it uses the word2number library (imported as w2n) to convert words like 'one hundred and fifty' into their numerical equivalent (e.g., 150.0).\n",
        "Symbolic Math Parsing: If word2number can't convert it, it then attempts to use the sympy library (imported as sp) to parse and evaluate mathematical expressions.\n",
        " This means it can understand and solve things like 'sqrt(4)' or 'pi / 2' and return their numerical result.\n",
        "Fallback Number Extraction: As a last resort, if all previous attempts fail, it looks for any numbers within the text (even if it's a jumbled string) and tries to extract them and return their average.\n",
        " If no numbers are found, it defaults to 0.0.\"\"\"\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def smart_normalize(answer_text: Any) -> float:\n",
        "    \"\"\"Normalize any math answer to float, supporting numbers, words, and expressions.\"\"\"\n",
        "    if answer_text is None:\n",
        "        logger.warning(\"Normalization failed: answer is None, returning 0.0\")\n",
        "        return 0.0\n",
        "    if isinstance(answer_text, (int, float)):\n",
        "        return float(answer_text)\n",
        "    try:\n",
        "        text = str(answer_text).strip().lower()\n",
        "        text = re.sub(r'[^\\w\\s.-]', '', text).replace(',', '')\n",
        "        # Try direct float conversion\n",
        "        if re.match(r'^-?\\d*\\.?\\d+$', text):\n",
        "            return float(text)\n",
        "        # Try word-to-number\n",
        "        try:\n",
        "            return float(w2n.word_to_num(text))\n",
        "        except ValueError:\n",
        "            pass\n",
        "        # Try symbolic parsing with comprehensive math functions\n",
        "        expr = sp.sympify(text, evaluate=False, locals={\n",
        "            'sin': sp.sin, 'cos': sp.cos, 'tan': sp.tan, 'cot': sp.cot, 'sec': sp.sec, 'csc': sp.csc,\n",
        "            'pi': sp.pi, 'sqrt': sp.sqrt, 'log': sp.log, 'ln': sp.ln, 'exp': sp.exp,\n",
        "            'arcsin': sp.asin, 'arccos': sp.acos, 'arctan': sp.atan,\n",
        "            'Integral': sp.Integral, 'Sum': sp.Sum, 'Product': sp.Product\n",
        "        })\n",
        "        sol = float(expr.evalf())\n",
        "        logger.debug(f\"Symbolic normalization succeeded: {text} -> {sol}\")\n",
        "        return sol\n",
        "    except (ValueError, TypeError, sp.SympifyError) as e:\n",
        "        logger.warning(f\"Normalization failed for {answer_text}: {e}, trying number extraction\")\n",
        "        try:\n",
        "            # Fallback: extract numbers and average\n",
        "            num_matches = [float(n) for n in sp.sympify(text).atoms(sp.Number) if isinstance(n, sp.Number)]\n",
        "            if num_matches:\n",
        "                sol = sum(num_matches) / len(num_matches)\n",
        "                logger.debug(f\"Fallback succeeded: mean of numbers {num_matches} -> {sol}\")\n",
        "                return sol\n",
        "            logger.warning(\"No numbers found, returning 0.0\")\n",
        "            return 0.0\n",
        "        except Exception as e2:\n",
        "            logger.warning(f\"Fallback failed: {e2}, returning 0.0\")\n",
        "            return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91f2-reMBT_6",
        "outputId": "d1900bea-a443-4c02-cdc2-cfefea80b399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=978771e9370ea0957453db825607427e61d206b0c00efdca18650e99f41dd1e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/79/fb/d25928e599c7e11fe4e00d32048cd74933f34a74c633d2aea6\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install word2number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qoczy-BO9cub",
        "outputId": "41c7c0ed-ed16-4408-a640-34184747d01d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/smart_preprocessor_v2.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/smart_preprocessor_v2.py\n",
        "import logging\n",
        "from typing import Dict, Optional, Tuple, Union, List\n",
        "from smart_utils import smart_normalize\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class SmartPreprocessorV2:\n",
        "    def __init__(self, context_col=None, target_col=None, debug=False):\n",
        "        self.context_col = context_col\n",
        "        self.target_col = target_col\n",
        "        self.debug = debug\n",
        "        self.min_context_len = 10  # Minimum char length to qualify as context\n",
        "\n",
        "    def is_number(self, val):\n",
        "        \"\"\"Check if value is numeric-like.\"\"\"\n",
        "        try:\n",
        "            float(val)\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def score_field(self, key, value) -> Tuple[float, float]:\n",
        "        \"\"\"Returns (context_score, target_score).\"\"\"\n",
        "        if value is None:\n",
        "            return (0, 0)\n",
        "        if isinstance(value, str):\n",
        "            length = len(value.strip())\n",
        "            if length > self.min_context_len and \" \" in value:\n",
        "                return (1.0, 0.0)  # Likely context\n",
        "            if self.is_number(value):\n",
        "                return (0.0, 1.0)  # Likely numeric target\n",
        "            return (0.5, 0.2)      # Possible mixed\n",
        "        if isinstance(value, (int, float)):\n",
        "            return (0.0, 1.0)\n",
        "        return (0.0, 0.0)\n",
        "\n",
        "    def autodetect_fields(self, sample: Dict):\n",
        "        \"\"\"Auto-detect best context and target columns.\"\"\"\n",
        "        field_scores = {}\n",
        "        for k, v in sample.items():\n",
        "            ctx_score, tgt_score = self.score_field(k, v)\n",
        "            field_scores[k] = (ctx_score, tgt_score)\n",
        "\n",
        "        sorted_fields = sorted(field_scores.items(), key=lambda kv: kv[1], reverse=True)\n",
        "        context_candidates = [f for f, (c, t) in sorted_fields if c > 0]\n",
        "        target_candidates = [f for f, (c, t) in sorted_fields if t > 0]\n",
        "\n",
        "        if context_candidates:\n",
        "            self.context_col = context_candidates[0]\n",
        "        if target_candidates:\n",
        "            self.target_col = target_candidates[0]\n",
        "\n",
        "        if self.debug:\n",
        "            logger.info(f\"[AutoDetect] Context → {self.context_col}, Target → {self.target_col}\")\n",
        "\n",
        "    def preprocess(self, sample: Dict) -> Optional[Dict]:\n",
        "        \"\"\"Return processed dict with id, context, target, equation.\"\"\"\n",
        "        if not self.context_col or not self.target_col:\n",
        "            self.autodetect_fields(sample)\n",
        "\n",
        "        sid = sample.get('id', 'unknown')\n",
        "\n",
        "        context_val = sample.get(self.context_col)\n",
        "        target_val = sample.get(self.target_col)\n",
        "\n",
        "        if not context_val or not target_val:\n",
        "            logger.warning(f\"[{sid}] Missing context or target → Skipping sample.\")\n",
        "            return None\n",
        "\n",
        "        # Handle numeric vs text\n",
        "        if self.is_number(target_val) or isinstance(target_val, (int, float)):\n",
        "            normalized_target = smart_normalize(target_val)\n",
        "        else:\n",
        "            try:\n",
        "                float_attempt = float(str(target_val).strip())\n",
        "                normalized_target = smart_normalize(target_val)\n",
        "            except ValueError:\n",
        "                target_str = str(target_val).strip()\n",
        "                # Auto-split multi-label text\n",
        "                if \",\" in target_str or \";\" in target_str:\n",
        "                    normalized_target = [lab.strip() for lab in target_str.replace(\";\", \",\").split(\",\") if lab.strip()]\n",
        "                else:\n",
        "                    normalized_target = target_str\n",
        "\n",
        "        return {\n",
        "            'id': sid,\n",
        "            'context': {self.context_col: str(context_val)},\n",
        "            'target': normalized_target,\n",
        "            'equation': sample.get('equation')\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeGGWFXE9dM8",
        "outputId": "ed5f0830-ad0f-416b-90d6-8bc9f3ef7eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tokenizer_and_embedding.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/tokenizer_and_embedding.py\n",
        "import torch, re, logging\n",
        "from typing import List\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TokenEmbedding:\n",
        "    def __init__(self, vocab: List[str], dim:int=50, device:str='cpu'):\n",
        "        self.dim, self.device = dim, device\n",
        "        self.vocab = ['<unk>'] + vocab\n",
        "        self.word2idx = {w:i for i,w in enumerate(self.vocab)}\n",
        "        self.embeddings = torch.randn(len(self.vocab), dim, device=device) / (dim**0.5)\n",
        "        logger.info(f\"TokenEmbedding: vocab_size={len(self.vocab)} dim={dim}\")\n",
        "\n",
        "    def lookup(self, token:str) -> torch.Tensor:\n",
        "        idx = self.word2idx.get(token, 0)\n",
        "        return self.embeddings[idx]\n",
        "\n",
        "def universal_tokenizer(text: str) -> List[str]:\n",
        "    if not text:\n",
        "        return []\n",
        "    # split numbers, identifiers, symbols\n",
        "    return re.findall(r'\\d+\\.\\d+|\\d+|[A-Za-z]+|[+\\-*/^=():]', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3cVFN619dkp",
        "outputId": "b6cd66b6-faf0-4954-c99d-df42bca98400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/target_processor.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/target_processor.py\n",
        "import torch\n",
        "import logging\n",
        "from typing import List, Union\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MultiLabelTargetProcessor:\n",
        "    \"\"\"\n",
        "    Handles multi-label targets for classification or regression.\n",
        "    - For text labels: builds a label→index map and returns multi-hot vectors.\n",
        "    - For numeric labels: returns float tensors directly.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.label_map = {}\n",
        "        self.max_value = 1.0\n",
        "        self.is_numeric_mode = False\n",
        "\n",
        "    def fit_labels(self, labels_list: List[Union[str, List[str], float, int]]):\n",
        "        \"\"\"\n",
        "        Fit label map from dataset.\n",
        "        labels_list is a list where each item is:\n",
        "          - a single label (str or float)\n",
        "          - OR a list of labels (multi-label case)\n",
        "        \"\"\"\n",
        "        for labels in labels_list:\n",
        "            if isinstance(labels, (float, int)):\n",
        "                self.is_numeric_mode = True\n",
        "                self.max_value = max(self.max_value, float(labels))\n",
        "            elif isinstance(labels, str):\n",
        "                self.label_map.setdefault(labels, len(self.label_map))\n",
        "            elif isinstance(labels, list):\n",
        "                for lab in labels:\n",
        "                    if isinstance(lab, (float, int)):\n",
        "                        self.is_numeric_mode = True\n",
        "                        self.max_value = max(self.max_value, float(lab))\n",
        "                    else:\n",
        "                        self.label_map.setdefault(str(lab), len(self.label_map))\n",
        "            else:\n",
        "                logger.warning(f\"Unsupported label type: {type(labels)}\")\n",
        "\n",
        "        logger.info(f\"Fitted label map: {self.label_map}\")\n",
        "        logger.info(f\"Numeric mode: {self.is_numeric_mode}, Max value: {self.max_value}\")\n",
        "\n",
        "    def encode(self, labels: Union[str, List[str], float, int]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Encode labels into tensor.\n",
        "        - If numeric mode: returns float tensor normalized by max_value.\n",
        "        - Else: returns multi-hot vector for label(s).\n",
        "        \"\"\"\n",
        "        if self.is_numeric_mode:\n",
        "            try:\n",
        "                return torch.tensor(float(labels) / self.max_value, dtype=torch.float32)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Numeric encoding failed for {labels}: {e}\")\n",
        "                return torch.tensor(0.0, dtype=torch.float32)\n",
        "\n",
        "        # Classification mode\n",
        "        vec = torch.zeros(len(self.label_map), dtype=torch.float32)\n",
        "        if isinstance(labels, str):\n",
        "            idx = self.label_map.get(labels)\n",
        "            if idx is not None:\n",
        "                vec[idx] = 1.0\n",
        "        elif isinstance(labels, list):\n",
        "            for lab in labels:\n",
        "                idx = self.label_map.get(str(lab))\n",
        "                if idx is not None:\n",
        "                    vec[idx] = 1.0\n",
        "        else:\n",
        "            logger.warning(f\"Unsupported label type for encoding: {type(labels)}\")\n",
        "\n",
        "        return vec\n",
        "\n",
        "    def decode(self, tensor: torch.Tensor) -> Union[str, float, List[str]]:\n",
        "        \"\"\"\n",
        "        Decode tensor back into labels or numeric value.\n",
        "        \"\"\"\n",
        "        if self.is_numeric_mode:\n",
        "            return tensor.item() * self.max_value\n",
        "\n",
        "        # Classification mode: return list of labels with value > 0.5\n",
        "        indices = (tensor > 0.5).nonzero(as_tuple=True)[0].tolist()\n",
        "        return [lab for lab, idx in self.label_map.items() if idx in indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2:"
      ],
      "metadata": {
        "id": "P9XWQwVtwLgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XW-WpeZWwLF_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHuRInHLLpws",
        "outputId": "51a048e6-de6a-47b2-d922-8e1fd89ca37f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/TreeNodeV1.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/TreeNodeV1.py\n",
        "import torch\n",
        "import logging\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(\"TreeNodeV1\")\n",
        "\n",
        "\n",
        "class TreeNodeV1:\n",
        "    \"\"\"\n",
        "    Minimal Tree Node (Shape-Free):\n",
        "    - NO shape normalization\n",
        "    - NO TLite logic\n",
        "    - shape_type stored raw with zero logic\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        value: str,\n",
        "        shape_type: Optional[str] = None,\n",
        "        vector: Optional[Any] = None,\n",
        "        confidence: float = 0.0,\n",
        "        spiral_index: Optional[int] = None,\n",
        "        id: Optional[str] = None,\n",
        "        shape_tag: Optional[str] = None,\n",
        "        level: int = 0,\n",
        "        max_children: int = 10,\n",
        "        label_text: Optional[str] = None,\n",
        "        label_embedding: Optional[Any] = None,\n",
        "        label_vector: Optional[Any] = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        self.value = value\n",
        "        self.id = id if id is not None else value\n",
        "\n",
        "        # raw, unmanaged shape tag\n",
        "        self.shape_type = shape_tag if shape_tag is not None else shape_type\n",
        "\n",
        "        self.level = int(level)\n",
        "        self.spiral_index = spiral_index\n",
        "        self.max_children = int(max_children)\n",
        "        self.children = []\n",
        "        self.confidence = float(confidence)\n",
        "\n",
        "        # Label info\n",
        "        self.label_text = label_text\n",
        "        self.label_embedding = None\n",
        "        self.label_vector = None\n",
        "\n",
        "        # Sub-features\n",
        "        self.sub_features: Dict[str, Any] = {}\n",
        "        self.feature_confidence: Dict[str, float] = {}\n",
        "\n",
        "        # vectors\n",
        "        self._assign_vector_safe(\"vector\", vector)\n",
        "        self._assign_vector_safe(\"label_embedding\", label_embedding, attr_name=\"label_embedding\")\n",
        "        self._assign_vector_safe(\"label_vector\", label_vector, attr_name=\"label_vector\")\n",
        "\n",
        "    # ---------------- INTERNAL HELPERS ----------------\n",
        "    def _assign_vector_safe(self, attr_label: str, value: Any, attr_name: Optional[str] = None):\n",
        "        if attr_name is None:\n",
        "            attr_name = attr_label\n",
        "        try:\n",
        "            if value is None:\n",
        "                setattr(self, attr_name, None)\n",
        "                return\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                setattr(self, attr_name, value.clone().detach().float())\n",
        "            else:\n",
        "                setattr(self, attr_name, torch.tensor(value, dtype=torch.float32))\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Vector assignment failed for {self.id}: {e}\")\n",
        "            setattr(self, attr_name, None)\n",
        "\n",
        "    # ---------------- API ----------------\n",
        "    def store_vector(self, vector: Any):\n",
        "        self._assign_vector_safe(\"vector\", vector)\n",
        "\n",
        "    def get_vector(self):\n",
        "        return getattr(self, \"vector\", None)\n",
        "\n",
        "    def get_confidence(self):\n",
        "        return float(self.confidence)\n",
        "\n",
        "    def add_child(self, child: \"TreeNodeV1\"):\n",
        "        if isinstance(child, TreeNodeV1):\n",
        "            if len(self.children) < self.max_children:\n",
        "                self.children.append(child)\n",
        "            else:\n",
        "                logger.warning(f\"Node {self.id}: max children limit reached\")\n",
        "        else:\n",
        "            logger.warning(f\"Invalid child added to {self.id}: not TreeNodeV1\")\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return len(self.children) == 0\n",
        "\n",
        "    # ---------------- SIMPLE SETTERS NEEDED BY PIPELINE ----------------\n",
        "    def set_spiral_index(self, idx: int):\n",
        "        self.spiral_index = idx\n",
        "\n",
        "    def set_label_text(self, text: str):\n",
        "        self.label_text = text\n",
        "\n",
        "    def set_label_embedding(self, embedding: Any):\n",
        "        self._assign_vector_safe(\"label_embedding\", embedding, attr_name=\"label_embedding\")\n",
        "\n",
        "    def set_label_vector(self, vector: Any):\n",
        "        self._assign_vector_safe(\"label_vector\", vector, attr_name=\"label_vector\")\n",
        "\n",
        "    def get_label_vector(self):\n",
        "        return getattr(self, \"label_vector\", None)\n",
        "\n",
        "    def get_label_embedding(self):\n",
        "        return getattr(self, \"label_embedding\", None)\n",
        "\n",
        "    # ---------------- SERIALIZATION ----------------\n",
        "    def to_dict(self):\n",
        "        def _maybe(x):\n",
        "            if isinstance(x, torch.Tensor):\n",
        "                try: return x.cpu().numpy().tolist()\n",
        "                except: return None\n",
        "            return x\n",
        "\n",
        "        return {\n",
        "            \"value\": self.value,\n",
        "            \"id\": self.id,\n",
        "            \"shape_type\": self.shape_type,\n",
        "            \"level\": self.level,\n",
        "            \"spiral_index\": self.spiral_index,\n",
        "            \"confidence\": self.confidence,\n",
        "            \"n_children\": len(self.children),\n",
        "            \"label_text\": self.label_text,\n",
        "            \"vector\": _maybe(self.get_vector()),\n",
        "            \"label_vector\": _maybe(self.get_label_vector()),\n",
        "            \"label_embedding\": _maybe(self.get_label_embedding()),\n",
        "            \"sub_features\": self.sub_features,\n",
        "            \"feature_confidence\": self.feature_confidence,\n",
        "        }\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (\n",
        "            f\"<TreeNodeV1 id={self.id!r} value={self.value!r} \"\n",
        "            f\"shape={self.shape_type!r} conf={self.confidence:.3f} \"\n",
        "            f\"children={len(self.children)}>\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile /content/tree_node_utils.py\n",
        "import logging\n",
        "import torch\n",
        "from typing import List\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logger = logging.getLogger(\"tree_node_utils\")\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def sanitize_raw_nodes(raw_nodes: List, raise_on_invalid=False):\n",
        "    \"\"\"\n",
        "    Keep only TreeNodeV1 instances with valid tensor vectors.\n",
        "    Returns sanitized list and a dict with counts.\n",
        "    \"\"\"\n",
        "    good = []\n",
        "    bad_count = 0\n",
        "    none_vector_count = 0\n",
        "    invalid_type_count = 0\n",
        "\n",
        "    for n in raw_nodes:\n",
        "        if isinstance(n, TreeNodeV1):\n",
        "            vec = getattr(n, \"vector\", None)\n",
        "            if vec is None:\n",
        "                none_vector_count += 1\n",
        "                logger.warning(\"sanitize_raw_nodes: Node %s skipped, vector is None\", getattr(n, \"id\", \"<no-id>\"))\n",
        "                continue\n",
        "            if not isinstance(vec, torch.Tensor):\n",
        "                try:\n",
        "                    n.store_vector(torch.tensor(vec, dtype=torch.float32))\n",
        "                except Exception:\n",
        "                    none_vector_count += 1\n",
        "                    logger.warning(\"sanitize_raw_nodes: Node %s skipped, vector conversion failed\", getattr(n, \"id\", \"<no-id>\"))\n",
        "                    continue\n",
        "            good.append(n)\n",
        "        else:\n",
        "            invalid_type_count += 1\n",
        "            bad_count += 1\n",
        "            logger.warning(\"sanitize_raw_nodes: Invalid node type skipped: %s\", type(n))\n",
        "\n",
        "    stats = {\n",
        "        \"good\": len(good),\n",
        "        \"invalid_type\": invalid_type_count,\n",
        "        \"none_vector\": none_vector_count\n",
        "    }\n",
        "    logger.info(\"sanitize_raw_nodes stats: %s\", stats)\n",
        "    if raise_on_invalid and (invalid_type_count > 0 or none_vector_count > 0):\n",
        "        raise ValueError(f\"sanitize_raw_nodes found invalid nodes: {stats}\")\n",
        "    return good, stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KoDAmOILlKS",
        "outputId": "e4e469b8-4354-493f-b1ee-7278928341d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tree_node_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I1pIbBaLp_9",
        "outputId": "0ec691c2-3f02-4b19-e3a9-74552ea3be49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/TreeBuilderV2.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/TreeBuilderV2.py\n",
        "import logging\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import torch\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(\"TreeBuilderV2\")\n",
        "\n",
        "\n",
        "class TreeBuilderV2:\n",
        "    \"\"\"\n",
        "    FINAL SHAPE-FREE VERSION\n",
        "    ------------------------\n",
        "    • Completely removes shape_type\n",
        "    • Nodes only store: value, vector, spiral_index, confidence, labels\n",
        "    • No TLite model, no shape abstraction, no tagging\n",
        "    • Pure tree builder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device: str = \"cpu\", dim: int = 50):\n",
        "        self.device = device\n",
        "        self.dim = dim\n",
        "\n",
        "    def _ensure_dim(self, vector: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Ensure vector is 1D tensor of length self.dim.\"\"\"\n",
        "        if not isinstance(vector, torch.Tensor):\n",
        "            try:\n",
        "                vector = torch.tensor(vector, dtype=torch.float32)\n",
        "            except Exception:\n",
        "                raise ValueError(\"Vector not convertible to torch.Tensor\")\n",
        "\n",
        "        if vector.dim() > 1:\n",
        "            vector = vector.view(-1)\n",
        "\n",
        "        v = vector.to(self.device).float()\n",
        "        current = v.shape[0]\n",
        "\n",
        "        if current == self.dim:\n",
        "            return v\n",
        "        elif current < self.dim:\n",
        "            pad = torch.zeros(self.dim - current, device=self.device)\n",
        "            return torch.cat([v, pad], dim=0)\n",
        "        else:\n",
        "            return v[:self.dim]\n",
        "\n",
        "    def build_tree(self,\n",
        "                   vec_pairs: List[Tuple[str, torch.Tensor]],\n",
        "                   label_texts: Optional[Dict[str, str]] = None,\n",
        "                   token_embedding=None,\n",
        "                   target_processor=None) -> Optional[TreeNodeV1]:\n",
        "\n",
        "        if not vec_pairs:\n",
        "            logger.warning(\"TreeBuilderV2: empty vec_pairs\")\n",
        "            return None\n",
        "\n",
        "        sample_id = vec_pairs[0][0]\n",
        "        root = TreeNodeV1(value=\"root\", id=sample_id, max_children=len(vec_pairs))\n",
        "\n",
        "        spiral_index = 0\n",
        "        for token, raw_vec in vec_pairs:\n",
        "            if raw_vec is None:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                vec = self._ensure_dim(raw_vec)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            node = TreeNodeV1(value=str(token))    # ❗ no shape assignment\n",
        "            node.set_spiral_index(spiral_index)\n",
        "            spiral_index += 1\n",
        "            node.store_vector(vec)\n",
        "\n",
        "            # confidence = normalized L2 norm (0–1)\n",
        "            try:\n",
        "                norm = float(torch.norm(vec).item())\n",
        "                node.confidence = min(max(norm / (self.dim ** 0.5 + 1e-8), 0.0), 1.0)\n",
        "            except:\n",
        "                node.confidence = 0.0\n",
        "\n",
        "            # Optional label features\n",
        "            if label_texts and token in label_texts:\n",
        "                label = label_texts[token]\n",
        "                node.set_label_text(label)\n",
        "\n",
        "                if token_embedding:\n",
        "                    try:\n",
        "                        node.set_label_embedding(token_embedding.lookup(label))\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                if target_processor:\n",
        "                    try:\n",
        "                        node.set_label_vector(target_processor.encode(label))\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "            root.add_child(node)\n",
        "\n",
        "        # compute root vector as mean of child vectors\n",
        "        child_vecs = [c.get_vector() for c in root.children if c.get_vector() is not None]\n",
        "\n",
        "        if child_vecs:\n",
        "            try:\n",
        "                stacked = torch.stack(child_vecs)\n",
        "                root_vec = stacked.mean(dim=0)\n",
        "                root.store_vector(root_vec)\n",
        "                root.confidence = float(sum(c.confidence for c in root.children) / len(root.children))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltGCQ65ML8f-",
        "outputId": "0bfc178c-3992-4816-bd83-58b40274de44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/TreeDecoder.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/TreeDecoder.py\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Dict, Any, Iterable, List, Optional\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"TreeDecoder\")\n",
        "\n",
        "\n",
        "class TreeDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Minimal, TLite-free TreeDecoder that:\n",
        "      - maps a tree root vector -> a post-processed vector via a small MLP (forward)\n",
        "      - provides `reconstruct_tree(nodes)` which returns a JSON-serializable dict\n",
        "        mapping node_id -> { vector: [...], children: [child_id,...], meta... }\n",
        "    Assumes node objects implement:\n",
        "      - .id (str or int)\n",
        "      - .get_vector() -> torch.Tensor | None\n",
        "      - .children -> iterable of child nodes\n",
        "      - optional attributes: .label_text, .shape_type, .confidence\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim: int, hidden_dim: int = 128, device: str = \"cpu\"):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = device\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim).to(device)\n",
        "        self.act = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim).to(device)\n",
        "\n",
        "    def forward(self, tree_root) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Accepts a TreeNode-like root. Uses root.get_vector() as input.\n",
        "        Returns a tensor of shape (dim,) on the configured device.\n",
        "        If input invalid or missing, returns zeros.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if tree_root is None:\n",
        "                logger.warning(\"TreeDecoder.forward received None root; returning zeros\")\n",
        "                return torch.zeros(self.dim, device=self.device)\n",
        "            vec = None\n",
        "            # prefer get_vector method if available\n",
        "            if hasattr(tree_root, \"get_vector\"):\n",
        "                vec = tree_root.get_vector()\n",
        "            else:\n",
        "                vec = getattr(tree_root, \"vector\", None)\n",
        "\n",
        "            if vec is None:\n",
        "                logger.warning(\"TreeDecoder.forward: root vector missing; returning zeros\")\n",
        "                return torch.zeros(self.dim, device=self.device)\n",
        "\n",
        "            if not isinstance(vec, torch.Tensor):\n",
        "                vec = torch.tensor(vec, dtype=torch.float32)\n",
        "\n",
        "            vec = vec.to(self.device).float()\n",
        "            # adjust length (pad/truncate) if needed\n",
        "            if vec.numel() != self.dim:\n",
        "                v = vec.view(-1)\n",
        "                if v.numel() < self.dim:\n",
        "                    pad = torch.zeros(self.dim - v.numel(), device=self.device)\n",
        "                    v = torch.cat([v.to(self.device), pad], dim=0)\n",
        "                else:\n",
        "                    v = v[: self.dim].to(self.device)\n",
        "                vec = v\n",
        "\n",
        "            x = self.fc1(vec)\n",
        "            x = self.act(x)\n",
        "            x = self.fc2(x)\n",
        "            if torch.isnan(x).any():\n",
        "                logger.error(\"TreeDecoder.forward produced NaNs; returning zeros\")\n",
        "                return torch.zeros(self.dim, device=self.device)\n",
        "            return x\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"TreeDecoder.forward failed: {e}\")\n",
        "            return torch.zeros(self.dim, device=self.device)\n",
        "\n",
        "    # ---------------------- Serialization utilities ----------------------\n",
        "    @staticmethod\n",
        "    def _tensor_to_list_safe(t: Optional[torch.Tensor]) -> Optional[List[float]]:\n",
        "        if t is None:\n",
        "            return None\n",
        "        try:\n",
        "            if not isinstance(t, torch.Tensor):\n",
        "                t = torch.tensor(t, dtype=torch.float32)\n",
        "            return t.detach().cpu().numpy().tolist()\n",
        "        except Exception:\n",
        "            # last resort: try to convert iterables\n",
        "            try:\n",
        "                return [float(x) for x in list(t)]\n",
        "            except Exception:\n",
        "                return None\n",
        "\n",
        "    def reconstruct_tree(self, nodes: Iterable) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Convert an iterable of nodes into a JSON-serializable mapping:\n",
        "          node_id -> {\n",
        "            \"value\": <node.value if present>,\n",
        "            \"vector\": [...],         # or None\n",
        "            \"children\": [child_id,...],\n",
        "            \"label_text\": ...,\n",
        "            \"shape_type\": ...,\n",
        "            \"confidence\": float,\n",
        "            \"meta\": {...}            # any sub_features if present (non-tensor)\n",
        "          }\n",
        "        NOTE: children are represented by their ids to avoid recursive nesting.\n",
        "        \"\"\"\n",
        "        result: Dict[str, Any] = {}\n",
        "        for obj in nodes:\n",
        "            try:\n",
        "                node_id = getattr(obj, \"id\", None)\n",
        "                if node_id is None:\n",
        "                    # fallback to value or str(obj)\n",
        "                    node_id = getattr(obj, \"value\", None) or str(obj)\n",
        "\n",
        "                node_id = str(node_id)\n",
        "\n",
        "                # vector\n",
        "                vec = None\n",
        "                if hasattr(obj, \"get_vector\"):\n",
        "                    try:\n",
        "                        vec = obj.get_vector()\n",
        "                    except Exception:\n",
        "                        vec = getattr(obj, \"vector\", None)\n",
        "                else:\n",
        "                    vec = getattr(obj, \"vector\", None)\n",
        "\n",
        "                vec_list = self._tensor_to_list_safe(vec)\n",
        "\n",
        "                # children -> list of ids (string)\n",
        "                children_ids: List[str] = []\n",
        "                try:\n",
        "                    chs = getattr(obj, \"children\", []) or []\n",
        "                    for c in chs:\n",
        "                        cid = getattr(c, \"id\", None) or getattr(c, \"value\", None) or str(c)\n",
        "                        children_ids.append(str(cid))\n",
        "                except Exception:\n",
        "                    children_ids = []\n",
        "\n",
        "                # optional metadata\n",
        "                label_text = getattr(obj, \"label_text\", None)\n",
        "                shape_type = getattr(obj, \"shape_type\", None)\n",
        "                confidence = None\n",
        "                try:\n",
        "                    confidence = float(getattr(obj, \"confidence\", None)) if getattr(obj, \"confidence\", None) is not None else None\n",
        "                except Exception:\n",
        "                    confidence = None\n",
        "\n",
        "                # collect non-tensor sub_features if present\n",
        "                meta = {}\n",
        "                try:\n",
        "                    subf = getattr(obj, \"sub_features\", None)\n",
        "                    if isinstance(subf, dict):\n",
        "                        for k, v in subf.items():\n",
        "                            # try safe convert tensors to lists; otherwise keep primitive\n",
        "                            if isinstance(v, torch.Tensor):\n",
        "                                meta[k] = self._tensor_to_list_safe(v)\n",
        "                            else:\n",
        "                                try:\n",
        "                                    # ensure JSON-serializable simple types\n",
        "                                    if isinstance(v, (str, int, float, bool, list, dict, type(None))):\n",
        "                                        meta[k] = v\n",
        "                                    else:\n",
        "                                        meta[k] = str(v)\n",
        "                                except Exception:\n",
        "                                    meta[k] = str(v)\n",
        "                except Exception:\n",
        "                    meta = {}\n",
        "\n",
        "                result[node_id] = {\n",
        "                    \"value\": getattr(obj, \"value\", None),\n",
        "                    \"vector\": vec_list,\n",
        "                    \"children\": children_ids,\n",
        "                    \"label_text\": label_text,\n",
        "                    \"shape_type\": shape_type,\n",
        "                    \"confidence\": confidence,\n",
        "                    \"meta\": meta,\n",
        "                }\n",
        "            except Exception as exc:\n",
        "                logger.debug(f\"reconstruct_tree: skipping node due to error: {exc}\")\n",
        "                continue\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhzysqeuOfbX",
        "outputId": "56240cd0-4532-4363-fdaf-d3d2a80988f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tree_pos_encoder.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/tree_pos_encoder.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import hashlib\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TreePosEncoder(nn.Module):\n",
        "    def __init__(self, pos_dim=16, max_positions=10000, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.pos_dim = pos_dim\n",
        "        self.max_positions = max_positions\n",
        "        self.path_embed = nn.Embedding(max_positions, pos_dim).to(device)\n",
        "        self.time_embed = nn.Embedding(100, pos_dim).to(device)\n",
        "\n",
        "    def path_to_index(self, path):\n",
        "        if isinstance(path, int):  # spiral_index\n",
        "            return path % self.max_positions\n",
        "        h = int(hashlib.md5(str(path).encode()).hexdigest(), 16)\n",
        "        return h % self.max_positions\n",
        "\n",
        "    def forward(self, access_path, timestamp: int = 0):\n",
        "        path_idx = self.path_to_index(access_path)\n",
        "        path_vec = self.path_embed(torch.tensor(path_idx, device=self.device))\n",
        "        time_idx = timestamp % 100\n",
        "        time_vec = self.time_embed(torch.tensor(time_idx, device=self.device))\n",
        "        return path_vec + time_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 3:"
      ],
      "metadata": {
        "id": "IuWB2WAkwZWw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE54s7ewEfr3",
        "outputId": "d8f95ad2-8eea-4f1e-b44f-cdcf285e2026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/grid_seed_opencv.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/grid_seed_opencv.py\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class GridDividerOpenCV:\n",
        "    def __init__(self, grid_rows=None, grid_cols=None, target_subpart_area=32*32):\n",
        "        self.grid_rows = grid_rows\n",
        "        self.grid_cols = grid_cols\n",
        "        self.target_subpart_area = target_subpart_area\n",
        "\n",
        "    def divide(self, image_np):\n",
        "        if image_np is None or len(image_np.shape) != 3 or image_np.shape[2] != 3:\n",
        "            logger.error(\"Invalid image: must be 3-channel BGR\")\n",
        "            return []\n",
        "        h, w, _ = image_np.shape\n",
        "        if self.grid_rows is None or self.grid_cols is None:\n",
        "            total_area = w * h\n",
        "            num_subparts = max(1, total_area // self.target_subpart_area)\n",
        "            side = max(1, int(math.sqrt(num_subparts)))\n",
        "            self.grid_rows = self.grid_cols = side\n",
        "        grid_h = max(1, h // self.grid_rows)\n",
        "        grid_w = max(1, w // self.grid_cols)\n",
        "        subparts = []\n",
        "        for r in range(self.grid_rows):\n",
        "            for c in range(self.grid_cols):\n",
        "                top = r * grid_h\n",
        "                left = c * grid_w\n",
        "                bottom = h if r == self.grid_rows - 1 else top + grid_h\n",
        "                right = w if c == self.grid_cols - 1 else left + grid_w\n",
        "                crop = image_np[top:bottom, left:right]\n",
        "                if crop.size == 0:\n",
        "                    logger.warning(f\"Empty crop at ({r}, {c})\")\n",
        "                    continue\n",
        "                subparts.append({\n",
        "                    \"coords\": (r, c),\n",
        "                    \"box\": (top, left, bottom, right),\n",
        "                    \"image\": crop\n",
        "                })\n",
        "        logger.info(f\"Divided image into {len(subparts)} subparts\")\n",
        "        return subparts\n",
        "\n",
        "class SeedSelectorOpenCV:\n",
        "    def __init__(self, method=\"sobel\"):\n",
        "        self.method = method\n",
        "\n",
        "    def _sobel_gradient(self, gray):\n",
        "        dx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "        dy = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "        grad = cv2.magnitude(dx, dy)\n",
        "        return grad\n",
        "\n",
        "    def select_seed(self, sub_image_np):\n",
        "        if sub_image_np is None or len(sub_image_np.shape) != 3:\n",
        "            logger.error(\"Invalid sub-image: must be 3-channel\")\n",
        "            return {\"seed\": (0, 0), \"metric_value\": 0.0}\n",
        "        gray = cv2.cvtColor(sub_image_np, cv2.COLOR_BGR2GRAY)\n",
        "        gray_small = cv2.resize(gray, (32, 32))\n",
        "        if self.method == \"sobel\":\n",
        "            metric = self._sobel_gradient(gray_small)\n",
        "        elif self.method == \"intensity\":\n",
        "            metric = gray_small.astype(np.float32)\n",
        "        else:\n",
        "            logger.error(f\"Unsupported method: {self.method}\")\n",
        "            return {\"seed\": (0, 0), \"metric_value\": 0.0}\n",
        "        idx = np.unravel_index(np.argmax(metric), metric.shape)\n",
        "        logger.info(f\"Selected seed at {idx} with metric {metric[idx]}\")\n",
        "        return {\n",
        "            \"seed\": idx,\n",
        "            \"metric_value\": float(metric[idx])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLsQILAMVPc-",
        "outputId": "dbd98515-a8c0-40d5-e0ba-7ed1c4c3e80e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/whirlpool_scanner_opencv.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/whirlpool_scanner_opencv.py\n",
        "import numpy as np\n",
        "import logging\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "import torch\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class WhirlpoolScannerOpenCV:\n",
        "    def __init__(self, color_threshold=30, max_nodes=100):\n",
        "        self.color_threshold = color_threshold\n",
        "        self.max_nodes = max_nodes\n",
        "\n",
        "    def scan(self, image_np, seed):\n",
        "        if image_np is None or len(image_np.shape) != 3 or image_np.shape[2] != 3:\n",
        "            logger.error(\"Invalid image: must be 3-channel BGR\")\n",
        "            return []\n",
        "        h, w, _ = image_np.shape\n",
        "        center_y, center_x = seed\n",
        "        if not (0 <= center_y < h and 0 <= center_x < w):\n",
        "            logger.error(f\"Invalid seed: {seed}, image size: ({h}, {w})\")\n",
        "            return []\n",
        "        visited = np.zeros((h, w), dtype=bool)\n",
        "        nodes = []\n",
        "\n",
        "        def in_bounds(y, x):\n",
        "            return 0 <= y < h and 0 <= x < w\n",
        "\n",
        "        def color_dist(c1, c2):\n",
        "            return np.linalg.norm(c1.astype(np.float32) - c2.astype(np.float32))\n",
        "\n",
        "        directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
        "        spiral_radius = 1\n",
        "        step = 1\n",
        "        cy, cx = center_y, center_x\n",
        "        origin_color = image_np[cy, cx]\n",
        "        q = [(cy, cx)]\n",
        "        visited[cy, cx] = True\n",
        "        node_pixels = [(cy, cx)]\n",
        "\n",
        "        while len(nodes) < self.max_nodes and step < max(h, w):\n",
        "            for d in directions:\n",
        "                for _ in range(spiral_radius):\n",
        "                    cy += d[0]\n",
        "                    cx += d[1]\n",
        "                    if not in_bounds(cy, cx) or visited[cy, cx]:\n",
        "                        continue\n",
        "                    pixel_color = image_np[cy, cx]\n",
        "                    if color_dist(pixel_color, origin_color) < self.color_threshold:\n",
        "                        q.append((cy, cx))\n",
        "                        visited[cy, cx] = True\n",
        "                        node_pixels.append((cy, cx))\n",
        "                    if len(node_pixels) >= 50:\n",
        "                        node = self._build_node(image_np, node_pixels, len(nodes))\n",
        "                        nodes.append(node)\n",
        "                        node_pixels = []\n",
        "                    if len(nodes) >= self.max_nodes:\n",
        "                        break\n",
        "                if len(nodes) >= self.max_nodes:\n",
        "                    break\n",
        "            step += 1\n",
        "            spiral_radius += 1\n",
        "\n",
        "        if node_pixels:\n",
        "            node = self._build_node(image_np, node_pixels, len(nodes))\n",
        "            nodes.append(node)\n",
        "\n",
        "        logger.info(f\"Whirlpool scan created {len(nodes)} nodes\")\n",
        "        for i, node in enumerate(nodes):\n",
        "           logger.info(f\"[Whirlpool] Node {i}: ID={node.id}, shape={node.shape_type}, vector_shape={None if node.get_vector() is None else tuple(node.get_vector().shape)}\")\n",
        "        return nodes\n",
        "\n",
        "    # -----------------------------\n",
        "    #  Shape descriptor functions\n",
        "    # -----------------------------\n",
        "    def _extract_patch(self, image_np, pixels, target_size=64, pad=4):\n",
        "        \"\"\"\n",
        "        Extract a tight patch around pixels, pad and resize to target_size (square).\n",
        "        \"\"\"\n",
        "        ys = [p[0] for p in pixels]\n",
        "        xs = [p[1] for p in pixels]\n",
        "        miny, maxy = max(0, min(ys)-pad), min(image_np.shape[0]-1, max(ys)+pad)\n",
        "        minx, maxx = max(0, min(xs)-pad), min(image_np.shape[1]-1, max(xs)+pad)\n",
        "        patch = image_np[miny:maxy+1, minx:maxx+1].copy()\n",
        "        if patch.size == 0:\n",
        "            patch = np.zeros((target_size, target_size, 3), dtype=np.uint8)\n",
        "        patch_gray = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)\n",
        "        # resize preserving aspect ratio, pad to square\n",
        "        h, w = patch_gray.shape\n",
        "        if h == 0 or w == 0:\n",
        "            patch_resized = np.zeros((target_size, target_size), dtype=np.uint8)\n",
        "        else:\n",
        "            scale = target_size / max(h, w)\n",
        "            new_h, new_w = max(1, int(h*scale)), max(1, int(w*scale))\n",
        "            resized = cv2.resize(patch_gray, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "            top = (target_size - new_h) // 2\n",
        "            left = (target_size - new_w) // 2\n",
        "            patch_resized = np.zeros((target_size, target_size), dtype=np.uint8)\n",
        "            patch_resized[top:top+new_h, left:left+new_w] = resized\n",
        "        return patch_resized\n",
        "\n",
        "    def _hog_descriptor(self, img, cells_y=2, cells_x=2, bins=8):\n",
        "        \"\"\"\n",
        "        Simple HOG-style descriptor: divide image into cells_y x cells_x,\n",
        "        compute gradient orientation hist in each cell with 'bins' bins.\n",
        "        Returns cells_y * cells_x * bins dims.\n",
        "        \"\"\"\n",
        "        # gradients\n",
        "        gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=3)\n",
        "        gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=3)\n",
        "        mag, ang = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
        "        ang = ang % 180.0  # unsigned gradients\n",
        "        h, w = img.shape\n",
        "        cell_h = h // cells_y\n",
        "        cell_w = w // cells_x\n",
        "        hist = []\n",
        "        for i in range(cells_y):\n",
        "            for j in range(cells_x):\n",
        "                y0, y1 = i*cell_h, (i+1)*cell_h if i < cells_y-1 else h\n",
        "                x0, x1 = j*cell_w, (j+1)*cell_w if j < cells_x-1 else w\n",
        "                mag_cell = mag[y0:y1, x0:x1].ravel()\n",
        "                ang_cell = ang[y0:y1, x0:x1].ravel()\n",
        "                if ang_cell.size == 0:\n",
        "                    hist_cell = np.zeros(bins, dtype=np.float32)\n",
        "                else:\n",
        "                    hist_cell, _ = np.histogram(ang_cell, bins=bins, range=(0,180), weights=mag_cell)\n",
        "                    if hist_cell.sum() > 0:\n",
        "                        hist_cell = hist_cell / (hist_cell.sum() + 1e-6)\n",
        "                hist.append(hist_cell)\n",
        "        return np.concatenate(hist).astype(np.float32)\n",
        "\n",
        "    def _radial_profile(self, img, bins=9):\n",
        "        \"\"\"\n",
        "        Radial profile: distances from centroid, histogram into 'bins'.\n",
        "        \"\"\"\n",
        "        h, w = img.shape\n",
        "        Y, X = np.indices((h, w))\n",
        "        mask = img > 0  # non-zero intensities as foreground\n",
        "        if not mask.any():\n",
        "            return np.zeros(bins, dtype=np.float32)\n",
        "        ys = Y[mask]; xs = X[mask]\n",
        "        cy, cx = ys.mean(), xs.mean()\n",
        "        dists = np.sqrt((ys - cy)**2 + (xs - cx)**2)\n",
        "        maxd = dists.max() if dists.size>0 else 1.0\n",
        "        hist, _ = np.histogram(dists, bins=bins, range=(0, maxd), weights=None)\n",
        "        if hist.sum() > 0:\n",
        "            hist = hist / (hist.sum() + 1e-6)\n",
        "        return hist.astype(np.float32)\n",
        "\n",
        "    def _contour_stats(self, img):\n",
        "        \"\"\"\n",
        "        Compute basic contour statistics from a binary threshold of img.\n",
        "        Returns: contour_count, mean_area_ratio, perimeter_area_ratio, solidity, eccentricity, compactness\n",
        "        \"\"\"\n",
        "        # adaptive threshold to emphasize shapes\n",
        "        _, th = cv2.threshold(img, 0, 255, cv2.THRESH_OTSU + cv2.THRESH_BINARY)\n",
        "        # close small holes\n",
        "        kernel = np.ones((3,3), np.uint8)\n",
        "        th = cv2.morphologyEx(th, cv2.MORPH_CLOSE, kernel)\n",
        "        contours, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if not contours:\n",
        "            return np.array([0., 0., 0., 0., 0., 0.], dtype=np.float32)\n",
        "        areas = np.array([cv2.contourArea(c) for c in contours], dtype=np.float32)\n",
        "        perims = np.array([cv2.arcLength(c, True) for c in contours], dtype=np.float32)\n",
        "        total_area = areas.sum()\n",
        "        img_area = img.shape[0] * img.shape[1]\n",
        "        mean_area_ratio = (areas.mean() / (img_area + 1e-6))\n",
        "        contour_count = float(len(contours))\n",
        "        # perimeter/area ratio (mean)\n",
        "        per_area = (perims / (areas + 1e-6)).mean()\n",
        "        # solidity: area / convex hull area mean\n",
        "        hull_areas = []\n",
        "        for c in contours:\n",
        "            hull = cv2.convexHull(c)\n",
        "            hull_areas.append(cv2.contourArea(hull) + 1e-6)\n",
        "        hull_areas = np.array(hull_areas, dtype=np.float32)\n",
        "        solidity = (areas / hull_areas).mean()\n",
        "        # eccentricity from moments (for largest contour)\n",
        "        largest_idx = int(np.argmax(areas))\n",
        "        c = contours[largest_idx]\n",
        "        mu = cv2.moments(c)\n",
        "        if mu['mu20'] + mu['mu02'] == 0:\n",
        "            eccentricity = 0.0\n",
        "        else:\n",
        "            common = math.sqrt((mu['mu20'] - mu['mu02'])**2 + 4*mu.get('mu11',0)**2)\n",
        "            l1 = (mu['mu20'] + mu['mu02'] + common) / 2.0\n",
        "            l2 = (mu['mu20'] + mu['mu02'] - common) / 2.0\n",
        "            if l1 <= 0:\n",
        "                eccentricity = 0.0\n",
        "            else:\n",
        "                eccentricity = float(math.sqrt(1 - (l2 / (l1 + 1e-9))))\n",
        "        # compactness: (perimeter^2) / (4*pi*area) mean\n",
        "        compact = ((perims**2) / (4 * math.pi * (areas + 1e-6))).mean()\n",
        "        return np.array([contour_count, mean_area_ratio, per_area, solidity, eccentricity, compact], dtype=np.float32)\n",
        "\n",
        "    def _edge_density(self, img):\n",
        "        edges = cv2.Canny(img, 50, 150)\n",
        "        return float(edges.sum()) / (img.size + 1e-6)\n",
        "\n",
        "    def _gradient_stats(self, img):\n",
        "        gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=3)\n",
        "        gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=3)\n",
        "        mag = np.sqrt(gx*gx + gy*gy)\n",
        "        return np.array([mag.mean(), mag.std()], dtype=np.float32)\n",
        "\n",
        "    # -----------------------------\n",
        "    #  _build_node replaced to produce 50-d descriptor\n",
        "    # -----------------------------\n",
        "    def _build_node(self, image_np, pixels, node_id):\n",
        "       patch = self._extract_patch(image_np, pixels, target_size=64, pad=4)\n",
        "\n",
        "       hog = self._hog_descriptor(patch, cells_y=2, cells_x=2, bins=8)\n",
        "       radial = self._radial_profile(patch, bins=9)\n",
        "       contour_feats = self._contour_stats(patch)\n",
        "       edge_d = np.array([self._edge_density(patch)], dtype=np.float32)\n",
        "       grad_stats = self._gradient_stats(patch)\n",
        "\n",
        "       feat = np.concatenate([hog, radial, contour_feats, edge_d, grad_stats], axis=0)\n",
        "\n",
        "       if feat.shape[0] != 50:\n",
        "           if feat.shape[0] < 50:\n",
        "               feat = np.concatenate([feat, np.zeros(50 - feat.shape[0], dtype=np.float32)])\n",
        "           else:\n",
        "               feat = feat[:50]\n",
        "\n",
        "       norm = np.linalg.norm(feat)\n",
        "       if norm > 0:\n",
        "           feat = feat / (norm + 1e-8)\n",
        "\n",
        "\n",
        "       node = TreeNodeV1(id=node_id, value=f\"node_{node_id}\", shape_type=None)\n",
        "       node.store_vector(torch.tensor(feat, dtype=torch.float32))\n",
        "       return node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9hw9u8Vnc8H",
        "outputId": "57b0e5ec-234f-4459-98c2-562ef13fed5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/parallel_whirlpool_processor_opencv.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/parallel_whirlpool_processor_opencv.py\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "import logging\n",
        "from grid_seed_opencv import GridDividerOpenCV, SeedSelectorOpenCV\n",
        "from whirlpool_scanner_opencv import WhirlpoolScannerOpenCV\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ParallelWhirlpoolProcessorOpenCV:\n",
        "    def __init__(self, grid_rows=None, grid_cols=None, target_area=1024, max_threads=1,\n",
        "                 color_thresh=30, max_nodes=50):\n",
        "        self.grid_rows = grid_rows\n",
        "        self.grid_cols = grid_cols\n",
        "        self.target_area = target_area\n",
        "        self.max_threads = max_threads\n",
        "        self.color_thresh = color_thresh\n",
        "        self.max_nodes = max_nodes\n",
        "        self.scanner = WhirlpoolScannerOpenCV(color_threshold=color_thresh, max_nodes=max_nodes)\n",
        "        self.selector = SeedSelectorOpenCV(method=\"sobel\")\n",
        "        logger.info(f\"Initialized ParallelWhirlpoolProcessor with {max_threads} threads\")\n",
        "\n",
        "    def process(self, image_np):\n",
        "        if image_np is None or len(image_np.shape) != 3:\n",
        "            logger.error(\"Invalid image: must be 3-channel\")\n",
        "            return []\n",
        "        divider = GridDividerOpenCV(grid_rows=self.grid_rows,\n",
        "                                    grid_cols=self.grid_cols,\n",
        "                                    target_subpart_area=self.target_area)\n",
        "        subparts = divider.divide(image_np)\n",
        "        results = []\n",
        "        for part in subparts:  # Single-thread for Colab stability\n",
        "            sub_img = part[\"image\"]\n",
        "            coords = part[\"coords\"]\n",
        "            try:\n",
        "                seed_info = self.selector.select_seed(sub_img)\n",
        "                seed_y, seed_x = seed_info[\"seed\"]\n",
        "                resized_h, resized_w = 32, 32\n",
        "                orig_h, orig_w, _ = sub_img.shape\n",
        "                scale_y = orig_h / resized_h\n",
        "                scale_x = orig_w / resized_w\n",
        "                scaled_y = min(int(seed_y * scale_y), orig_h - 1)\n",
        "                scaled_x = min(int(seed_x * scale_x), orig_w - 1)\n",
        "                nodes = self.scanner.scan(sub_img, seed=(scaled_y, scaled_x))\n",
        "                results.append({\n",
        "                    \"coords\": coords,\n",
        "                    \"seed\": (scaled_y, scaled_x),\n",
        "                    \"nodes\": nodes,\n",
        "                    \"metric_value\": seed_info[\"metric_value\"]\n",
        "                })\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing subpart {coords}: {e}\")\n",
        "                results.append({\"coords\": coords, \"error\": str(e)})\n",
        "        logger.info(f\"Processed {len(results)} subparts\")\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYPwDS2s25D9",
        "outputId": "08c659e1-7e8c-4909-e402-c847a4b75095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/whirlpool_node_standardizer.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/whirlpool_node_standardizer.py\n",
        "import torch\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class WhirlpoolNodeStandardizer:\n",
        "    def __init__(self, target_dim=16, device='cpu', max_norm=255.0):\n",
        "        self.target_dim = target_dim\n",
        "        self.device = device\n",
        "        self.max_norm = max_norm\n",
        "\n",
        "    def _normalize_vector(self, vec: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Normalize any vector to target_dim, no shape-tag logic.\"\"\"\n",
        "        if vec is None:\n",
        "            return torch.zeros(self.target_dim, device=self.device)\n",
        "\n",
        "        vec = vec.to(self.device).float()\n",
        "\n",
        "        if torch.isnan(vec).any() or torch.isinf(vec).any():\n",
        "            return torch.zeros(self.target_dim, device=self.device)\n",
        "\n",
        "        # If RGB or image feature vector\n",
        "        vec = vec / (self.max_norm + 1e-8)\n",
        "\n",
        "        # Resize\n",
        "        if vec.shape[0] == self.target_dim:\n",
        "            return vec\n",
        "        elif vec.shape[0] < self.target_dim:\n",
        "            padded = torch.zeros(self.target_dim, device=self.device)\n",
        "            padded[:vec.shape[0]] = vec\n",
        "            return padded\n",
        "        else:\n",
        "            return vec[:self.target_dim]\n",
        "\n",
        "    def clean_node(self, node):\n",
        "        \"\"\"Remove empty or broken nodes, keep only vector-based validation.\"\"\"\n",
        "        if node is None:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            vec = node.get_vector()\n",
        "            if vec is None:\n",
        "                logger.warning(f\"Node {getattr(node, 'id', '?')} skipped: no vector\")\n",
        "                return None\n",
        "\n",
        "            # Normalize vector\n",
        "            vec_norm = self._normalize_vector(vec)\n",
        "            node.store_vector(vec_norm)\n",
        "\n",
        "            # Default simple attributes\n",
        "            if not hasattr(node, \"spiral_index\"):\n",
        "                node.spiral_index = -1\n",
        "            if not hasattr(node, \"level\"):\n",
        "                node.level = 0\n",
        "\n",
        "            # Reject all-zero nodes\n",
        "            if torch.allclose(vec_norm, torch.zeros_like(vec_norm)):\n",
        "                logger.warning(f\"Node {getattr(node, 'id', '?')} skipped: zero-vector\")\n",
        "                return None\n",
        "\n",
        "            return node\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to clean node: {e}\")\n",
        "            return None\n",
        "\n",
        "    def standardize_batch(self, nodes):\n",
        "        cleaned = []\n",
        "        for node in nodes:\n",
        "            c = self.clean_node(node)\n",
        "            if c is not None:\n",
        "                cleaned.append(c)\n",
        "        logger.info(f\"Standardized {len(cleaned)} / {len(nodes)} nodes\")\n",
        "        return cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my5Tbz8EswcP",
        "outputId": "6d7f80b6-c35e-4d04-b05e-7b9226d99d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/focused_grid_whirlpool_processor.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/focused_grid_whirlpool_processor.py\n",
        "import cv2\n",
        "import logging\n",
        "from grid_seed_opencv import GridDividerOpenCV, SeedSelectorOpenCV\n",
        "from parallel_whirlpool_processor_opencv import ParallelWhirlpoolProcessorOpenCV\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FocusedGridWhirlpoolProcessor:\n",
        "    def __init__(self, grid_rows=14, grid_cols=14, top_k=40, seed_method=\"sobel\"):\n",
        "        self.grid_rows = grid_rows\n",
        "        self.grid_cols = grid_cols\n",
        "        self.top_k = top_k\n",
        "        self.grid_divider = GridDividerOpenCV(grid_rows=self.grid_rows, grid_cols=self.grid_cols)\n",
        "        self.seed_selector = SeedSelectorOpenCV(method=seed_method)\n",
        "        self.whirlpool = ParallelWhirlpoolProcessorOpenCV()\n",
        "\n",
        "    def process(self, image_np):\n",
        "        if image_np is None or len(image_np.shape) != 3:\n",
        "            logger.error(\"Invalid input image.\")\n",
        "            return []\n",
        "\n",
        "        subparts = self.grid_divider.divide(image_np)\n",
        "        if not subparts:\n",
        "            logger.warning(\"No subparts extracted.\")\n",
        "            return []\n",
        "\n",
        "        # Score each subpart by its seed metric\n",
        "        scored = []\n",
        "        for part in subparts:\n",
        "            result = self.seed_selector.select_seed(part[\"image\"])\n",
        "            scored.append({\n",
        "                \"coords\": part[\"coords\"],\n",
        "                \"box\": part[\"box\"],\n",
        "                \"image\": part[\"image\"],\n",
        "                \"metric\": result[\"metric_value\"],\n",
        "                \"seed\": result[\"seed\"]\n",
        "            })\n",
        "\n",
        "        # Select top-K segments\n",
        "        top_segments = sorted(scored, key=lambda x: x[\"metric\"], reverse=True)[:self.top_k]\n",
        "        logger.info(f\"Selected top {self.top_k} of {len(scored)} subparts for focused processing\")\n",
        "\n",
        "        # Process selected segments with Whirlpool\n",
        "        results = []\n",
        "        for seg in top_segments:\n",
        "            whirl_result = self.whirlpool.process(seg[\"image\"])\n",
        "            results.extend(whirl_result)\n",
        "\n",
        "        logger.info(f\"Extracted {len(results)} node groups from top segments.\")\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmgvwH-TLw8Q",
        "outputId": "04c2e378-fd2d-4311-f5fd-1854f7a126cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/parallel_grid_seed_whirlpool_processor.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/parallel_grid_seed_whirlpool_processor.py\n",
        "import cv2\n",
        "import logging\n",
        "from grid_seed_opencv import GridDividerOpenCV, SeedSelectorOpenCV\n",
        "from parallel_whirlpool_processor_opencv import ParallelWhirlpoolProcessorOpenCV\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ParallelGridSeedWhirlpoolProcessor:\n",
        "    \"\"\"\n",
        "    SAFE VERSION (No multiprocessing)\n",
        "    ---------------------------------\n",
        "    • Processes subparts sequentially (but very lightweight)\n",
        "    • 100% compatible with all existing pipeline code\n",
        "    • Works reliably inside Google Colab / Jupyter\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, grid_rows=14, grid_cols=14, top_k=40, seed_method=\"sobel\"):\n",
        "        self.grid_rows = grid_rows\n",
        "        self.grid_cols = grid_cols\n",
        "        self.top_k = top_k\n",
        "        self.grid_divider = GridDividerOpenCV(grid_rows=grid_rows, grid_cols=grid_cols)\n",
        "        self.seed_selector = SeedSelectorOpenCV(method=seed_method)\n",
        "        self.whirlpool = ParallelWhirlpoolProcessorOpenCV()\n",
        "\n",
        "    def _score_segment_safe(self, part):\n",
        "        \"\"\"Local function — no pickling, no multiprocessing.\"\"\"\n",
        "        try:\n",
        "            res = self.seed_selector.select_seed(part[\"image\"])\n",
        "            return {\n",
        "                \"coords\": part[\"coords\"],\n",
        "                \"box\": part[\"box\"],\n",
        "                \"image\": part[\"image\"],\n",
        "                \"metric\": res[\"metric_value\"],\n",
        "                \"seed\": res[\"seed\"]\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Segment scoring failure: {e}\")\n",
        "            return None\n",
        "\n",
        "    def process(self, image_np):\n",
        "        if image_np is None or len(image_np.shape) != 3:\n",
        "            logger.error(\"Invalid input image.\")\n",
        "            return []\n",
        "\n",
        "        # Step 1: Divide into grid\n",
        "        subparts = self.grid_divider.divide(image_np)\n",
        "        if not subparts:\n",
        "            logger.warning(\"No subparts extracted.\")\n",
        "            return []\n",
        "\n",
        "        logger.info(f\"Scoring {len(subparts)} segments (SAFE single-thread)...\")\n",
        "\n",
        "        # Step 2: Score each segment safely\n",
        "        scored = []\n",
        "        for part in subparts:\n",
        "            score = self._score_segment_safe(part)\n",
        "            if score:\n",
        "                scored.append(score)\n",
        "\n",
        "        # Step 3: Take top-K by saliency metric\n",
        "        top_segments = sorted(scored, key=lambda x: x[\"metric\"], reverse=True)[:self.top_k]\n",
        "        logger.info(f\"Selected top {self.top_k} of {len(scored)} subparts\")\n",
        "\n",
        "        # Step 4: Whirlpool feature extraction\n",
        "        results = []\n",
        "        for seg in top_segments:\n",
        "            try:\n",
        "                whirl = self.whirlpool.process(seg[\"image\"])\n",
        "                results.extend(whirl)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Whirlpool error: {e}\")\n",
        "\n",
        "        logger.info(f\"Extracted {len(results)} node groups from top segments.\")\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH4x_4UtlCn1",
        "outputId": "ee4c5c97-fdde-498a-fb03-5f0112cfafd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/ReverseWhirlpoolCleanerV2.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/ReverseWhirlpoolCleanerV2.py\n",
        "\"\"\"\n",
        "ReverseWhirlpoolCleanerV2 (PURE SHAPE-FREE VERSION)\n",
        "---------------------------------------------------\n",
        "Cleans and merges nodes ONLY using:\n",
        " - vector norm\n",
        " - cosine similarity\n",
        " - confidence\n",
        "\n",
        "All references to shape_type, TLite, idx mappings are removed.\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "from typing import Optional, List\n",
        "import torch\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(\"ReverseWhirlpoolCleanerV2\")\n",
        "\n",
        "\n",
        "def _cosine_sim(a: torch.Tensor, b: torch.Tensor) -> float:\n",
        "    try:\n",
        "        a = a.view(-1).float()\n",
        "        b = b.view(-1).float()\n",
        "        denom = (torch.norm(a) * torch.norm(b)).item() + 1e-8\n",
        "        return float((a @ b).item() / denom)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "class ReverseWhirlpoolCleanerV2:\n",
        "    def __init__(\n",
        "        self,\n",
        "        sim_threshold: float = 0.85,\n",
        "        min_norm: float = 1e-3,\n",
        "        device: str = \"cpu\"\n",
        "    ):\n",
        "        self.sim_threshold = float(sim_threshold)\n",
        "        self.min_norm = float(min_norm)\n",
        "        self.device = device\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # STEP 1 — FILTER BAD NODES\n",
        "    # ---------------------------------------------------\n",
        "    def _valid_children(self, children: List[TreeNodeV1]) -> List[TreeNodeV1]:\n",
        "        valid = []\n",
        "        for c in children:\n",
        "            v = c.get_vector()\n",
        "            if v is None:\n",
        "                continue\n",
        "            try:\n",
        "                n = float(torch.norm(v).item())\n",
        "                if n < self.min_norm:\n",
        "                    continue\n",
        "            except Exception:\n",
        "                continue\n",
        "            valid.append(c)\n",
        "        return valid\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # STEP 2 — MERGE SIMILAR NODES\n",
        "    # ---------------------------------------------------\n",
        "    def _merge_similar(self, nodes: List[TreeNodeV1]) -> List[TreeNodeV1]:\n",
        "        merged = []\n",
        "        for n in nodes:\n",
        "            nv = n.get_vector()\n",
        "            if nv is None:\n",
        "                continue\n",
        "\n",
        "            placed = False\n",
        "            for rep in merged:\n",
        "                sim = _cosine_sim(nv, rep.get_vector())\n",
        "                if sim >= self.sim_threshold:\n",
        "                    try:\n",
        "                        w1 = float(rep.get_confidence())\n",
        "                        w2 = float(n.get_confidence())\n",
        "                        total = w1 + w2 if (w1 + w2) > 1e-8 else 1.0\n",
        "\n",
        "                        new_vec = (rep.get_vector() * w1 + nv * w2) / total\n",
        "                        rep.store_vector(new_vec)\n",
        "\n",
        "                        rep.confidence = max(rep.confidence, n.confidence)\n",
        "\n",
        "                        if not rep.label_text and n.label_text:\n",
        "                            rep.set_label_text(n.label_text)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                    placed = True\n",
        "                    break\n",
        "\n",
        "            if not placed:\n",
        "                merged.append(n)\n",
        "\n",
        "        return merged\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # STEP 3 — MAIN CLEAN FUNCTION\n",
        "    # ---------------------------------------------------\n",
        "    def clean(self, root: TreeNodeV1) -> Optional[TreeNodeV1]:\n",
        "        if root is None:\n",
        "            return None\n",
        "\n",
        "        if not hasattr(root, \"children\"):\n",
        "            return root\n",
        "\n",
        "        # 1. prune\n",
        "        valid = self._valid_children(root.children)\n",
        "        if not valid:\n",
        "            logger.info(\"Cleaner: no valid children\")\n",
        "            return TreeNodeV1(value=root.value, shape_type=\"group\", id=root.id)\n",
        "\n",
        "        # 2. merge\n",
        "        merged = self._merge_similar(valid)\n",
        "\n",
        "        # 3. new root (shape-free → we still keep 'group' as harmless label)\n",
        "        new_root = TreeNodeV1(\n",
        "            value=root.value,\n",
        "            shape_type=\"group\",\n",
        "            id=root.id,\n",
        "            max_children=len(merged)\n",
        "        )\n",
        "\n",
        "        # attach children\n",
        "        for i, n in enumerate(merged):\n",
        "            n.set_spiral_index(i)\n",
        "            new_root.add_child(n)\n",
        "\n",
        "        # 4. recompute root vector\n",
        "        child_vecs = [c.get_vector() for c in new_root.children if c.get_vector() is not None]\n",
        "        if child_vecs:\n",
        "            try:\n",
        "                new_root.store_vector(torch.stack(child_vecs).mean(dim=0))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return new_root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cSujNQ_lEim",
        "outputId": "6bb90c5a-f2bb-4abc-dcf9-9186f73fc75e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/utils_positional.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/utils_positional.py\n",
        "import torch\n",
        "import logging\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def assign_positional_encoding(node, pos_encoder, temp_encoder, path=\"root\", timestamp=0, device='cpu'):\n",
        "    \"\"\"\n",
        "    Add positional + temporal encodings into node vectors in-place.\n",
        "\n",
        "    - node: TreeNodeV1\n",
        "    - pos_encoder: callable(access_path, timestamp) -> torch.Tensor (dim == node.vector dim or compatible)\n",
        "    - temp_encoder: callable(timestamp) -> torch.Tensor (dim == node.vector dim or compatible)\n",
        "    - path: path id (used only if spiral_index missing)\n",
        "    - timestamp: integer time-step\n",
        "    - device: 'cpu' or 'cuda'\n",
        "    \"\"\"\n",
        "    if not isinstance(node, TreeNodeV1):\n",
        "        logger.warning(f\"assign_positional_encoding: invalid node type: {type(node)}\")\n",
        "        return\n",
        "\n",
        "    node_vec = node.get_vector()\n",
        "    if node_vec is None:\n",
        "        logger.warning(f\"assign_positional_encoding: node has no vector: {getattr(node,'id',None)}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # determine access path (prefer spiral_index if set)\n",
        "        path_value = getattr(node, 'spiral_index', path)\n",
        "        pos_vec = pos_encoder(path_value, timestamp)\n",
        "        temp_vec = temp_encoder(timestamp)\n",
        "\n",
        "        # ensure tensors and device\n",
        "        if not isinstance(pos_vec, torch.Tensor):\n",
        "            pos_vec = torch.tensor(pos_vec, dtype=torch.float32)\n",
        "        if not isinstance(temp_vec, torch.Tensor):\n",
        "            temp_vec = torch.tensor(temp_vec, dtype=torch.float32)\n",
        "\n",
        "        pos_vec = pos_vec.to(device)\n",
        "        temp_vec = temp_vec.to(device)\n",
        "        node_vec = node_vec.to(device)\n",
        "\n",
        "        # if encoders produce same dim as node vector, just add\n",
        "        if pos_vec.shape[0] == node_vec.shape[0] and temp_vec.shape[0] == node_vec.shape[0]:\n",
        "            node.store_vector(node_vec + pos_vec + temp_vec)\n",
        "        else:\n",
        "            # fallback: concatenate and crop to node vector length\n",
        "            cat = torch.cat([node_vec, pos_vec, temp_vec], dim=0)\n",
        "            node.store_vector(cat[: node_vec.shape[0]])\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"assign_positional_encoding: failed for node {getattr(node,'id', None)}: {e}\")\n",
        "\n",
        "    # recursive\n",
        "    for i, child in enumerate(node.children):\n",
        "        try:\n",
        "            assign_positional_encoding(child, pos_encoder, temp_encoder, f\"{path}.{i}\", timestamp + 1, device)\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"assign_positional_encoding: child {i} failed: {e}\")\n",
        "\n",
        "    logger.debug(f\"Assigned positional and temporal encoding to node: {node.value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-Yvm7ru_662",
        "outputId": "94799ff1-9766-4ff7-aaa0-85075f14da68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/explainability.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/explainability.py\n",
        "import torch\n",
        "import logging\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ExplainabilityModule:\n",
        "    def __init__(self, device='cpu'):\n",
        "        self.device = device\n",
        "\n",
        "    def explain(self, node, depth=0):\n",
        "        \"\"\"\n",
        "        Return list of human-readable explanation strings for the tree.\n",
        "        \"\"\"\n",
        "        if not isinstance(node, TreeNodeV1):\n",
        "            logger.error(f\"ExplainabilityModule.explain: Invalid node type: {type(node)}\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            conf = node.get_confidence() if hasattr(node, \"get_confidence\") else float(getattr(node, \"confidence\", 0.0))\n",
        "            explanation = [f\"Depth {depth}: Node {node.value}, Confidence {conf:.4f}, Shape {getattr(node,'shape_type', 'N/A')}\"]\n",
        "        except Exception:\n",
        "            explanation = [f\"Depth {depth}: Node {node.value}\"]\n",
        "\n",
        "        for child in node.children:\n",
        "            explanation.extend(self.explain(child, depth + 1))\n",
        "        return explanation\n",
        "\n",
        "    def visualize(self, node):\n",
        "        \"\"\"\n",
        "        Log the explanation and return it.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            explanation = self.explain(node)\n",
        "            if explanation:\n",
        "                logger.info(\"Tree Explanation:\\n\" + \"\\n\".join(explanation))\n",
        "            else:\n",
        "                logger.info(\"Tree Explanation: <empty>\")\n",
        "            return explanation\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ExplainabilityModule.visualize failed: {e}\")\n",
        "            return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMu_T59UNNQU",
        "outputId": "4a851e37-b512-4cde-b958-d7ae632db081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/positional_encoder.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/positional_encoder.py\n",
        "import torch\n",
        "\n",
        "class PositionalEncoder:\n",
        "    \"\"\"\n",
        "    Device-aware positional encoder that returns a vector of length `dim`.\n",
        "    Uses a vectorized implementation compatible with torch operations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim=50, device='cpu'):\n",
        "        self.dim = int(dim)\n",
        "        self.device = device\n",
        "\n",
        "    def _path_to_index(self, access_path):\n",
        "        # Convert path like 'root.0.1.2' → numerical index (e.g., combine digits).\n",
        "        if isinstance(access_path, str):\n",
        "            parts = access_path.strip().split('.')\n",
        "            nums = [int(p) for p in parts if p.isdigit()]\n",
        "            if nums:\n",
        "                # deterministic combination\n",
        "                idx = 0\n",
        "                for n in nums:\n",
        "                    idx = idx * 31 + (n + 1)\n",
        "                return idx\n",
        "        elif isinstance(access_path, int):\n",
        "            return int(access_path)\n",
        "        return 0\n",
        "\n",
        "    def __call__(self, access_path, timestamp=0):\n",
        "        \"\"\"\n",
        "        Return positional encoding tensor (torch.Tensor) on configured device.\n",
        "        The encoding uses a standard sin/cos scheme vectorized with torch.\n",
        "        \"\"\"\n",
        "        index = self._path_to_index(access_path)\n",
        "        # combine index and timestamp to provide time-varying encoding\n",
        "        pos_val = float(index + int(timestamp))\n",
        "\n",
        "        device = self.device\n",
        "        dim = self.dim\n",
        "\n",
        "        pe = torch.zeros(dim, dtype=torch.float32, device=device)\n",
        "\n",
        "        # positions scalar as tensor\n",
        "        pos = torch.tensor([pos_val], dtype=torch.float32, device=device)\n",
        "\n",
        "        # create denominators for even indices\n",
        "        inv_freq = torch.pow(10000.0, (torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim))\n",
        "\n",
        "        # compute vectorized sin/cos\n",
        "        angles = pos / inv_freq  # shape (dim/2,)\n",
        "        pe[0:dim:2] = torch.sin(angles)\n",
        "        pe[1:dim:2] = torch.cos(angles[: pe[1:dim:2].shape[0]])\n",
        "\n",
        "        return pe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRpmbCL6_7EV",
        "outputId": "ee5a640d-d2a3-4ed6-ad8d-c7b939b48e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/temporal_encoder.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/temporal_encoder.py\n",
        "import torch\n",
        "\n",
        "class TemporalEncoder:\n",
        "    \"\"\"\n",
        "    Simple sinusoidal temporal encoder (dimension = dim).\n",
        "    Produces a time-dependent encoding similar to transformer time embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim=16, device='cpu'):\n",
        "        self.dim = int(dim)\n",
        "        self.device = device\n",
        "        self.scale = torch.sqrt(torch.tensor(float(dim), dtype=torch.float32, device=device))\n",
        "\n",
        "    def forward(self, timestamp: int):\n",
        "        try:\n",
        "            t = torch.tensor(float(timestamp), dtype=torch.float32, device=self.device)\n",
        "\n",
        "            # even indices 0,2,4...\n",
        "            idx = torch.arange(0, self.dim, 2, dtype=torch.float32, device=self.device)\n",
        "\n",
        "            # denominator term\n",
        "            div_term = torch.exp(idx * (-torch.log(torch.tensor(10000.0, device=self.device)) / self.dim))\n",
        "\n",
        "            encoding = torch.zeros(self.dim, dtype=torch.float32, device=self.device)\n",
        "            encoding[0::2] = torch.sin(t * div_term)\n",
        "            encoding[1::2] = torch.cos(t * div_term)\n",
        "\n",
        "            return encoding / self.scale\n",
        "        except Exception:\n",
        "            # safe fallback\n",
        "            return torch.zeros(self.dim, dtype=torch.float32, device=self.device)\n",
        "\n",
        "    def __call__(self, timestamp):\n",
        "        return self.forward(timestamp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOiiK-k6TDjJ",
        "outputId": "1825d171-2845-4e75-815c-66e7abdb0471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/confidence_injector.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/confidence_injector.py\n",
        "import torch\n",
        "import logging\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"ConfidenceInjector\")\n",
        "\n",
        "\n",
        "class ConfidenceInjector:\n",
        "    \"\"\"\n",
        "    Injects confidence scores into nodes based on vector statistics.\n",
        "    mode = \"norm\": confidence = normalized vector norm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mode=\"norm\", device=\"cpu\"):\n",
        "        self.mode = mode\n",
        "        self.device = device\n",
        "\n",
        "    def inject(self, node):\n",
        "        if not isinstance(node, TreeNodeV1):\n",
        "            logger.warning(f\"ConfidenceInjector: invalid node type {type(node)}\")\n",
        "            return\n",
        "\n",
        "        vec = node.get_vector()\n",
        "\n",
        "        if vec is None:\n",
        "            node.confidence = 0.0\n",
        "        else:\n",
        "            vec = vec.to(self.device)\n",
        "\n",
        "            if self.mode == \"norm\":\n",
        "                # vector norm normalized to [0,1]\n",
        "                raw = float(torch.norm(vec).item())\n",
        "                conf = raw / 10.0      # scale factor, adjustable\n",
        "                conf = max(0.0, min(conf, 1.0))\n",
        "            else:\n",
        "                conf = 0.5   # fallback constant\n",
        "\n",
        "            node.confidence = round(conf, 4)\n",
        "\n",
        "        # recurse into children\n",
        "        for child in node.children:\n",
        "            self.inject(child)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab0b2ed-3415-4c99-898a-bd417cf13a4e",
        "id": "E7p0RIFxxTJ_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/TreeCNNppRunner.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/TreeCNNppRunner.py\n",
        "import cv2\n",
        "import torch\n",
        "import logging\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "from TreeBuilderV2 import TreeBuilderV2\n",
        "from ReverseWhirlpoolCleanerV2 import ReverseWhirlpoolCleanerV2\n",
        "from utils_positional import assign_positional_encoding\n",
        "from positional_encoder import PositionalEncoder\n",
        "from temporal_encoder import TemporalEncoder\n",
        "\n",
        "# Extractors (optional imports)\n",
        "try:\n",
        "    from parallel_grid_seed_whirlpool_processor import ParallelGridSeedWhirlpoolProcessor\n",
        "except Exception:\n",
        "    ParallelGridSeedWhirlpoolProcessor = None\n",
        "\n",
        "try:\n",
        "    from focused_grid_whirlpool_processor import FocusedGridWhirlpoolProcessor\n",
        "except Exception:\n",
        "    FocusedGridWhirlpoolProcessor = None\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"TreeCNNppRunner\")\n",
        "\n",
        "\n",
        "def _safe_extract_vec_pairs(nodes: List) -> List[Tuple[str, torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Accepts a list of TreeNodeV1-like objects and returns [(token, tensor), ...].\n",
        "    Ignores items without get_vector / vector or with invalid vectors.\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    skipped = 0\n",
        "    for i, n in enumerate(nodes or []):\n",
        "        try:\n",
        "            token = getattr(n, \"value\", f\"node_{i}\")\n",
        "            # prefer API get_vector(), else attribute 'vector'\n",
        "            vec = None\n",
        "            if hasattr(n, \"get_vector\"):\n",
        "                vec = n.get_vector()\n",
        "            elif hasattr(n, \"vector\"):\n",
        "                vec = getattr(n, \"vector\")\n",
        "            if vec is None:\n",
        "                skipped += 1\n",
        "                continue\n",
        "            if not isinstance(vec, torch.Tensor):\n",
        "                vec = torch.tensor(vec, dtype=torch.float32)\n",
        "            pairs.append((str(token), vec))\n",
        "        except Exception:\n",
        "            skipped += 1\n",
        "            continue\n",
        "    if skipped:\n",
        "        logger.debug(f\"_safe_extract_vec_pairs: skipped {skipped} invalid items\")\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def _collect_raw_nodes(extractor_output) -> List:\n",
        "    \"\"\"\n",
        "    Normalize extractor output to a flat list of node-like objects.\n",
        "    Accepts:\n",
        "      - list of dicts with key \"nodes\" (list)\n",
        "      - list of TreeNodeV1 objects\n",
        "      - single dict containing nodes\n",
        "    \"\"\"\n",
        "    raw = []\n",
        "    if extractor_output is None:\n",
        "        return raw\n",
        "\n",
        "    # If extractor returned a list, iterate\n",
        "    if isinstance(extractor_output, list):\n",
        "        for part in extractor_output:\n",
        "            if isinstance(part, dict):\n",
        "                nodes = part.get(\"nodes\")\n",
        "                if isinstance(nodes, list):\n",
        "                    raw.extend(nodes)\n",
        "                else:\n",
        "                    # some extractors may place nodes directly under different keys\n",
        "                    maybe = part.get(\"node\") or part.get(\"data\")\n",
        "                    if isinstance(maybe, list):\n",
        "                        raw.extend(maybe)\n",
        "            else:\n",
        "                # assume it's a node-like object\n",
        "                raw.append(part)\n",
        "    elif isinstance(extractor_output, dict):\n",
        "        nodes = extractor_output.get(\"nodes\")\n",
        "        if isinstance(nodes, list):\n",
        "            raw.extend(nodes)\n",
        "    else:\n",
        "        # single node-like object\n",
        "        raw.append(extractor_output)\n",
        "\n",
        "    return raw\n",
        "\n",
        "\n",
        "def run_treecnnpp(image_path: str,\n",
        "                  device: str = \"cpu\",\n",
        "                  dim: int = 50,\n",
        "                  use_parallel: bool = False) -> Optional[object]:\n",
        "    \"\"\"\n",
        "    Shape-free runner:\n",
        "      - read image\n",
        "      - run extractor (parallel or focused)\n",
        "      - build pre-tree from node vectors\n",
        "      - clean (norm + merge)\n",
        "      - assign positional/temporal encodings\n",
        "      - rebuild final tree and return it\n",
        "    \"\"\"\n",
        "    # read image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        logger.error(f\"TreeCNNppRunner: failed to read image '{image_path}'\")\n",
        "        return None\n",
        "\n",
        "    # choose extractor\n",
        "    extractor = None\n",
        "    if use_parallel and ParallelGridSeedWhirlpoolProcessor is not None:\n",
        "        try:\n",
        "            extractor = ParallelGridSeedWhirlpoolProcessor()\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to init Parallel extractor: {e}\")\n",
        "            extractor = None\n",
        "\n",
        "    if extractor is None and FocusedGridWhirlpoolProcessor is not None:\n",
        "        try:\n",
        "            extractor = FocusedGridWhirlpoolProcessor()\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to init Focused extractor: {e}\")\n",
        "            extractor = None\n",
        "\n",
        "    if extractor is None:\n",
        "        logger.error(\"No extractor available (install parallel_grid_seed_whirlpool_processor or focused_grid_whirlpool_processor).\")\n",
        "        return None\n",
        "\n",
        "    # extract\n",
        "    try:\n",
        "        extractor_output = extractor.process(image)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Extractor.process failed: {e}\")\n",
        "        return None\n",
        "\n",
        "    raw_nodes = _collect_raw_nodes(extractor_output)\n",
        "    if not raw_nodes:\n",
        "        logger.warning(\"Extractor returned no raw nodes\")\n",
        "        return None\n",
        "\n",
        "    vec_pairs = _safe_extract_vec_pairs(raw_nodes)\n",
        "    if not vec_pairs:\n",
        "        logger.warning(\"No valid (token,vector) pairs after extraction\")\n",
        "        return None\n",
        "\n",
        "    # Build pre-tree\n",
        "    builder = TreeBuilderV2(device=device, dim=dim)\n",
        "    try:\n",
        "        pre_tree = builder.build_tree(vec_pairs)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Pre-tree build failed: {e}\")\n",
        "        return None\n",
        "\n",
        "    if pre_tree is None:\n",
        "        logger.warning(\"Pre-tree construction returned None\")\n",
        "        return None\n",
        "\n",
        "    # Clean (shape-free)\n",
        "    cleaner = ReverseWhirlpoolCleanerV2(sim_threshold=0.85, min_norm=1e-3, device=device)\n",
        "    try:\n",
        "        cleaned = cleaner.clean(pre_tree)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Cleaner failed: {e}\")\n",
        "        return None\n",
        "\n",
        "    if cleaned is None:\n",
        "        logger.warning(\"Cleaner returned empty tree\")\n",
        "        return None\n",
        "\n",
        "    # Positional + temporal encoding (pass named device to avoid arg-order bugs)\n",
        "    pos_encoder = PositionalEncoder(dim)\n",
        "    temp_encoder = TemporalEncoder(dim)\n",
        "    try:\n",
        "        assign_positional_encoding(cleaned, pos_encoder, temp_encoder, path=\"root\", timestamp=0, device=device)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"assign_positional_encoding failed: {e}\")\n",
        "\n",
        "    # Rebuild final tree from cleaned children (if any)\n",
        "    final_pairs = _safe_extract_vec_pairs(getattr(cleaned, \"children\", []))\n",
        "    if not final_pairs:\n",
        "        # nothing to rebuild; return cleaned root\n",
        "        return cleaned\n",
        "\n",
        "    try:\n",
        "        final_tree = builder.build_tree(final_pairs)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Final build failed, returning cleaned tree: {e}\")\n",
        "        return cleaned\n",
        "\n",
        "    return final_tree\n",
        "\n",
        "\n",
        "# simple CLI\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"Usage: python TreeCNNppRunner.py path/to/image.jpg\")\n",
        "        sys.exit(1)\n",
        "    out = run_treecnnpp(sys.argv[1], device='cpu', dim=50, use_parallel=False)\n",
        "    print(\"Result:\", getattr(out, \"value\", None), \"children:\", len(getattr(out, \"children\", [])))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section"
      ],
      "metadata": {
        "id": "tbWmrgPuwlTm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUCmP4ekbz4R",
        "outputId": "d1646187-0869-431b-fd80-b77476e62177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/lazy_recursive.c\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/lazy_recursive.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#ifdef __cplusplus\n",
        "extern \"C\" {\n",
        "#endif\n",
        "\n",
        "typedef struct { float* data; int size; int* spiral_indices; int index_count; } Vector;\n",
        "\n",
        "Vector* lazy_recursive(float* input, int size, int* spiral_indices, int index_count) {\n",
        "    if (!input || !spiral_indices || size <= 0 || index_count <= 0) return NULL;\n",
        "    Vector* result = (Vector*)malloc(sizeof(Vector));\n",
        "    result->size = size;\n",
        "    result->data = (float*)calloc(size, sizeof(float));\n",
        "    result->spiral_indices = (int*)malloc(index_count * sizeof(int));\n",
        "    result->index_count = index_count;\n",
        "\n",
        "    for (int i = 0; i < index_count; i++) {\n",
        "        int idx = spiral_indices[i];\n",
        "        if (idx < size) {\n",
        "            result->data[idx] = input[idx] * 2.0;\n",
        "            result->spiral_indices[i] = idx;\n",
        "        }\n",
        "    }\n",
        "    return result;\n",
        "}\n",
        "\n",
        "void free_vector(Vector* vec) {\n",
        "    if (vec) {\n",
        "        free(vec->data);\n",
        "        free(vec->spiral_indices);\n",
        "        free(vec);\n",
        "    }\n",
        "}\n",
        "\n",
        "#ifdef __cplusplus\n",
        "}\n",
        "#endif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeXEkQMob0Dd",
        "outputId": "f61c30be-16f9-4a42-9a8d-1c0de050dfe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tree_pruner.c\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/tree_pruner.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#ifdef __cplusplus\n",
        "extern \"C\" {\n",
        "#endif\n",
        "\n",
        "typedef struct { int id; float confidence; float* vector; int vec_size; } Node;\n",
        "\n",
        "void tree_pruner(Node* nodes, int* size, float threshold) {\n",
        "    if (!nodes || !size || *size <= 0) return;\n",
        "    int new_size = 0;\n",
        "    for (int i = 0; i < *size; i++) {\n",
        "        if (nodes[i].confidence >= threshold) {\n",
        "            nodes[new_size] = nodes[i];\n",
        "            new_size++;\n",
        "        } else {\n",
        "            free(nodes[i].vector);\n",
        "        }\n",
        "    }\n",
        "    *size = new_size;\n",
        "}\n",
        "\n",
        "#ifdef __cplusplus\n",
        "}\n",
        "#endif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlK-oleHb0M0",
        "outputId": "9c11d721-8ed6-48e6-aa82-42762e1e700a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tree_matrix_hybrid.c\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/tree_matrix_hybrid.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#ifdef __cplusplus\n",
        "extern \"C\" {\n",
        "#endif\n",
        "\n",
        "typedef struct { float* data; int rows; int cols; int* indices; int nnz; } SparseMatrix;\n",
        "\n",
        "SparseMatrix* tree_matrix_hybrid(float* node_data, int node_count, int dim, int* sparse_indices, int nnz) {\n",
        "    if (!node_data || !sparse_indices || node_count <= 0 || dim <= 0 || nnz <= 0) return NULL;\n",
        "    SparseMatrix* matrix = (SparseMatrix*)malloc(sizeof(SparseMatrix));\n",
        "    matrix->rows = node_count;\n",
        "    matrix->cols = dim;\n",
        "    matrix->nnz = nnz > node_count * dim ? node_count * dim : nnz; // Cap nnz\n",
        "    matrix->indices = (int*)malloc(matrix->nnz * sizeof(int));\n",
        "    matrix->data = (float*)calloc(matrix->nnz, sizeof(float));\n",
        "\n",
        "    for (int i = 0; i < matrix->nnz; i++) {\n",
        "        int idx = sparse_indices[i % matrix->nnz];\n",
        "        if (idx < node_count * dim) {\n",
        "            matrix->data[i] = node_data[idx];\n",
        "            matrix->indices[i] = idx;\n",
        "        }\n",
        "    }\n",
        "    return matrix;\n",
        "}\n",
        "\n",
        "void free_matrix(SparseMatrix* matrix) {\n",
        "    if (matrix) {\n",
        "        free(matrix->data);\n",
        "        free(matrix->indices);\n",
        "        free(matrix);\n",
        "    }\n",
        "}\n",
        "\n",
        "#ifdef __cplusplus\n",
        "}\n",
        "#endif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTuUG1uwb0VN",
        "outputId": "a47c9c52-98c3-47e1-fb4f-e68b37b8c9fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/quantization.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/quantization.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <stdint.h>\n",
        "\n",
        "#ifdef __cplusplus\n",
        "extern \"C\" {\n",
        "#endif\n",
        "\n",
        "typedef enum { INT8, BFLOAT16, FLOAT16, FLOAT32 } QuantMode;\n",
        "\n",
        "QuantMode select_quant_mode(int contour_count, const char* user_flag) {\n",
        "    if (user_flag && strcmp(user_flag, \"--accuracy=high\") == 0) return FLOAT32;\n",
        "    if (user_flag && strcmp(user_flag, \"--efficiency=high\") == 0) return INT8;\n",
        "    if (contour_count <= 10) return INT8; // Simple shapes\n",
        "    if (contour_count <= 20) return BFLOAT16; // Medium complexity\n",
        "    return FLOAT16; // Complex images\n",
        "}\n",
        "\n",
        "void quantize(float* input, int size, int contour_count, const char* user_flag, void* output) {\n",
        "    if (!input || !output || size <= 0) return;\n",
        "    QuantMode mode = select_quant_mode(contour_count, user_flag);\n",
        "\n",
        "    float max_val = 0.0f;\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        if (isnan(input[i]) || isinf(input[i])) input[i] = 0.0f;\n",
        "        float abs_val = fabsf(input[i]);\n",
        "        if (abs_val > max_val) max_val = abs_val;\n",
        "    }\n",
        "\n",
        "    float scale = (max_val > 1e-6f && max_val < 1e6f) ? max_val / 127.0f : 1.0f;\n",
        "    float* out = (float*)output;\n",
        "\n",
        "    switch (mode) {\n",
        "        case INT8: {\n",
        "            for (int i = 0; i < size; i++) {\n",
        "                int8_t q = (int8_t)(roundf(input[i] / scale));\n",
        "                out[i] = q * scale;\n",
        "            }\n",
        "            break;\n",
        "        }\n",
        "        case BFLOAT16: {\n",
        "            for (int i = 0; i < size; i++) {\n",
        "                uint32_t f32 = *(uint32_t*)&input[i];\n",
        "                uint16_t b16 = (uint16_t)(f32 >> 16);\n",
        "                uint32_t restored = ((uint32_t)b16) << 16;\n",
        "                out[i] = *(float*)&restored;\n",
        "            }\n",
        "            break;\n",
        "        }\n",
        "        case FLOAT16: {\n",
        "            for (int i = 0; i < size; i++) {\n",
        "                uint32_t f32 = *(uint32_t*)&input[i];\n",
        "                uint16_t sign = (f32 >> 31) & 0x1;\n",
        "                int16_t exp = ((f32 >> 23) & 0xFF) - 127 + 15;\n",
        "                uint16_t mantissa = (f32 >> 13) & 0x3FF;\n",
        "\n",
        "                if (exp > 31) exp = 31;\n",
        "                if (exp < 0) exp = 0;\n",
        "\n",
        "                uint16_t f16 = (sign << 15) | ((uint16_t)exp << 10) | mantissa;\n",
        "\n",
        "                // convert back to float32\n",
        "                uint32_t restored = ((uint32_t)(f16 & 0x8000) << 16) |\n",
        "                                    (((uint32_t)((f16 >> 10) & 0x1F) + (127 - 15)) << 23) |\n",
        "                                    ((uint32_t)(f16 & 0x3FF) << 13);\n",
        "                out[i] = *(float*)&restored;\n",
        "            }\n",
        "            break;\n",
        "        }\n",
        "        case FLOAT32: {\n",
        "            memcpy(out, input, size * sizeof(float));\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "#ifdef __cplusplus\n",
        "}\n",
        "#endif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uunUoFcjUKOP"
      },
      "outputs": [],
      "source": [
        "!g++ -O3 -fPIC -shared /content/quantization.c -o /content/quantization.so"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnD6pCRI_3WH",
        "outputId": "65352e79-2487-4a69-9313-4ff6633eeca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/quantization_wrapper.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/quantization_wrapper.py\n",
        "import ctypes\n",
        "import torch\n",
        "import numpy as np\n",
        "import logging\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"QuantizationWrapper\")\n",
        "\n",
        "# ---------- Python fallback quantizers ----------\n",
        "def _py_quantize_int8(vec: np.ndarray):\n",
        "    # symmetric int8 with scale chosen per-vector\n",
        "    max_val = float(np.max(np.abs(vec))) if vec.size > 0 else 1.0\n",
        "    scale = max_val / 127.0 if max_val > 1e-6 and max_val < 1e6 else 1.0\n",
        "    q = np.round(vec / scale).astype(np.int8)\n",
        "    return (q.astype(np.float32) * scale), scale\n",
        "\n",
        "def _py_quantize_bfloat16(vec: np.ndarray):\n",
        "    # naive bf16-like: zero-out low 16 bits of float32 mantissa via view+mask\n",
        "    out = vec.astype(np.float32).copy()\n",
        "    # use numpy view for bit-level ops\n",
        "    u = out.view(np.uint32)\n",
        "    # drop low 16 bits (keep top 16 bits -> bfloat16 restore style)\n",
        "    u[:] = (u & 0xFFFF0000)\n",
        "    out = u.view(np.float32)\n",
        "    return out\n",
        "\n",
        "def _py_quantize_float16(vec: np.ndarray):\n",
        "    # use numpy's float16 conversion (may be slower but correct)\n",
        "    return vec.astype(np.float16).astype(np.float32)\n",
        "\n",
        "# ---------- Wrapper class ----------\n",
        "class QuantizationWrapper:\n",
        "    def __init__(self, dim=64, so_path=\"./quantization.so\", allow_fallback=True):\n",
        "        self.dim = dim\n",
        "        self.so_path = so_path\n",
        "        self.lib = None\n",
        "        self.allow_fallback = bool(allow_fallback)\n",
        "\n",
        "        # try to load the C shared library but do NOT raise on failure\n",
        "        try:\n",
        "            self.lib = ctypes.CDLL(self.so_path)\n",
        "            # set argtypes/restype for safety (keeps previous signature)\n",
        "            self.lib.quantize.argtypes = [\n",
        "                ctypes.POINTER(ctypes.c_float),  # input\n",
        "                ctypes.c_int,                    # size\n",
        "                ctypes.c_int,                    # contour_count\n",
        "                ctypes.c_char_p,                 # user_flag\n",
        "                ctypes.c_void_p                  # output\n",
        "            ]\n",
        "            self.lib.quantize.restype = None\n",
        "            logger.info(f\"Loaded quantization library from {self.so_path}\")\n",
        "        except Exception as e:\n",
        "            self.lib = None\n",
        "            logger.warning(f\"Could not load quantization library '{self.so_path}': {e}. Using Python fallback.\")\n",
        "\n",
        "    def _detect_mode(self, user_flag: str, contour_count: int):\n",
        "        # match the C select_quant_mode heuristic\n",
        "        if user_flag and user_flag == \"--accuracy=high\":\n",
        "            return \"FLOAT32\"\n",
        "        if user_flag and user_flag == \"--efficiency=high\":\n",
        "            return \"INT8\"\n",
        "        if contour_count <= 10:\n",
        "            return \"INT8\"\n",
        "        if contour_count <= 20:\n",
        "            return \"BFLOAT16\"\n",
        "        return \"FLOAT16\"\n",
        "\n",
        "    def apply_quantization(self, node: TreeNodeV1, user_flag=\"--efficiency=high\"):\n",
        "        \"\"\"\n",
        "        Quantize node vectors in-place (recurses into children).\n",
        "        If the C library is present, uses it; otherwise uses a safe Python fallback.\n",
        "        \"\"\"\n",
        "        if node is None:\n",
        "            return\n",
        "\n",
        "        vec_t = node.get_vector()\n",
        "        if vec_t is None:\n",
        "            # nothing to do, walk children\n",
        "            for ch in node.children:\n",
        "                self.apply_quantization(ch, user_flag=user_flag)\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            vec = vec_t.detach().cpu().numpy().astype(np.float32)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to convert node vector to numpy for node {getattr(node, 'id', '?')}: {e}\")\n",
        "            return\n",
        "\n",
        "        # sanitize\n",
        "        vec[np.isnan(vec)] = 0.0\n",
        "        vec[np.isinf(vec)] = 0.0\n",
        "\n",
        "        contour_count = 10\n",
        "        try:\n",
        "            if hasattr(node, \"get_contour_count\") and callable(getattr(node, \"get_contour_count\")):\n",
        "                contour_count = int(node.get_contour_count())\n",
        "            elif hasattr(node, \"contour_count\"):\n",
        "                contour_count = int(getattr(node, \"contour_count\"))\n",
        "        except Exception:\n",
        "            contour_count = 10\n",
        "\n",
        "        mode = self._detect_mode(user_flag, contour_count)\n",
        "\n",
        "        # If we have C library loaded, prefer it but guard with try/except\n",
        "        if self.lib is not None:\n",
        "            try:\n",
        "                size = int(len(vec))\n",
        "                # prepare output buffer matching float32 array (C code writes floats back)\n",
        "                OutputArrayType = ctypes.c_float * size\n",
        "                output_buf = OutputArrayType()\n",
        "                self.lib.quantize(\n",
        "                    vec.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                    ctypes.c_int(size),\n",
        "                    ctypes.c_int(contour_count),\n",
        "                    ctypes.c_char_p(user_flag.encode() if user_flag else b\"\"),\n",
        "                    ctypes.cast(output_buf, ctypes.c_void_p)\n",
        "                )\n",
        "                quantized = np.frombuffer(output_buf, dtype=np.float32).copy()\n",
        "                # sanity check\n",
        "                if np.isnan(quantized).any() or np.isinf(quantized).any():\n",
        "                    logger.warning(f\"Quantized vector for node {node.id} contains NaNs/Infs — falling back to Python quantizer\")\n",
        "                    raise RuntimeError(\"C quantize produced invalid values\")\n",
        "                node.store_vector(torch.tensor(quantized, dtype=torch.float32))\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"C quantization failed for node {getattr(node,'id','?')}: {e}\")\n",
        "                # fall back to Python quantization if allowed\n",
        "                if not self.allow_fallback:\n",
        "                    return\n",
        "                self._py_quantize_assign(node, vec, mode)\n",
        "        else:\n",
        "            # no C lib; use Python fallback\n",
        "            self._py_quantize_assign(node, vec, mode)\n",
        "\n",
        "        # recurse\n",
        "        for ch in node.children:\n",
        "            self.apply_quantization(ch, user_flag=user_flag)\n",
        "\n",
        "    def _py_quantize_assign(self, node, vec: np.ndarray, mode: str):\n",
        "        try:\n",
        "            if mode == \"INT8\":\n",
        "                qvec, scale = _py_quantize_int8(vec)\n",
        "                node.store_vector(torch.tensor(qvec, dtype=torch.float32))\n",
        "            elif mode == \"BFLOAT16\":\n",
        "                qvec = _py_quantize_bfloat16(vec)\n",
        "                node.store_vector(torch.tensor(qvec, dtype=torch.float32))\n",
        "            elif mode == \"FLOAT16\":\n",
        "                qvec = _py_quantize_float16(vec)\n",
        "                node.store_vector(torch.tensor(qvec, dtype=torch.float32))\n",
        "            else:  # FLOAT32\n",
        "                node.store_vector(torch.tensor(vec.astype(np.float32), dtype=torch.float32))\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Python fallback quantization failed for node {getattr(node,'id','?')}: {e}\")\n",
        "            # as last resort store zeros of same shape\n",
        "            node.store_vector(torch.zeros_like(node.get_vector() if node.get_vector() is not None else torch.zeros(self.dim)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuIX5PYYBXfg",
        "outputId": "1f73dc05-2e62-42f3-9f0c-39218b0e5de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/attention_aggregator.c\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/attention_aggregator.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "#ifdef __cplusplus\n",
        "extern \"C\" {\n",
        "#endif\n",
        "\n",
        "void attention_aggregate(float* vectors, int num_vectors, int dim, float* output) {\n",
        "    if (!vectors || !output || num_vectors <= 0 || dim <= 0) return;\n",
        "\n",
        "    float* weights = (float*)calloc(num_vectors, sizeof(float));\n",
        "    float sum = 0.0;\n",
        "\n",
        "    for (int i = 0; i < num_vectors; i++) {\n",
        "        weights[i] = expf(vectors[i * dim]);\n",
        "\n",
        "        if (isnan(weights[i]) || isinf(weights[i])) {\n",
        "            weights[i] = 0.0f;\n",
        "        }\n",
        "\n",
        "        sum += weights[i];\n",
        "    }\n",
        "\n",
        "    if (sum == 0.0f) {\n",
        "        for (int i = 0; i < num_vectors; i++) {\n",
        "            weights[i] = 1.0f / num_vectors;\n",
        "        }\n",
        "    } else {\n",
        "        for (int i = 0; i < num_vectors; i++) {\n",
        "            weights[i] /= sum;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for (int j = 0; j < dim; j++) {\n",
        "        output[j] = 0.0;\n",
        "        for (int i = 0; i < num_vectors; i++) {\n",
        "            output[j] += weights[i] * vectors[i * dim + j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    free(weights);\n",
        "}\n",
        "\n",
        "#ifdef __cplusplus\n",
        "}\n",
        "#endif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LKokY9kBXr0",
        "outputId": "39513f8b-e07d-4181-8da0-a41654895511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/attention_aggregator_wrapper.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/attention_aggregator_wrapper.py\n",
        "import ctypes\n",
        "import torch\n",
        "import logging\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"attention_aggregator_wrapper\")\n",
        "\n",
        "\n",
        "def _python_attention_aggregate(vectors: List[np.ndarray], dim: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Safe Python fallback attention aggregation:\n",
        "      - compute simple attention weights from the first element of each vector (stable)\n",
        "      - normalize weights, then weighted-sum vectors\n",
        "      - avoids exp overflow and NaNs\n",
        "    \"\"\"\n",
        "    if not vectors:\n",
        "        return torch.zeros(dim)\n",
        "    arr = np.stack([np.asarray(v, dtype=np.float32).reshape(-1)[:dim] for v in vectors], axis=0)\n",
        "    # score by mean energy of each vector (stable)\n",
        "    scores = np.clip(np.nan_to_num(arr.mean(axis=1)), -1e3, 1e3)\n",
        "    # shift and softmax in a stable way\n",
        "    max_s = scores.max() if scores.size else 0.0\n",
        "    exp_scores = np.exp(scores - max_s)\n",
        "    if exp_scores.sum() == 0:\n",
        "        weights = np.ones_like(exp_scores) / len(exp_scores)\n",
        "    else:\n",
        "        weights = exp_scores / (exp_scores.sum() + 1e-12)\n",
        "    out = (weights[:, None] * arr).sum(axis=0)\n",
        "    out = np.nan_to_num(out)\n",
        "    return torch.tensor(out, dtype=torch.float32)\n",
        "\n",
        "\n",
        "class AttentionAggregatorWrapper:\n",
        "    def __init__(self, dim: int = 64, device: str = 'cpu'):\n",
        "        self.dim = dim\n",
        "        self.device = device\n",
        "        self.lib = None\n",
        "        self._load_shared_lib()\n",
        "\n",
        "    def _load_shared_lib(self):\n",
        "        try:\n",
        "            self.lib = ctypes.CDLL('./attention_aggregator.so')\n",
        "            self.lib.attention_aggregate.argtypes = [\n",
        "                ctypes.POINTER(ctypes.c_float), ctypes.c_int, ctypes.c_int,\n",
        "                ctypes.POINTER(ctypes.c_float)\n",
        "            ]\n",
        "            self.lib.attention_aggregate.restype = None\n",
        "            logger.info(\"Loaded attention_aggregator.so successfully.\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not load attention_aggregator.so — using Python fallback. ({e})\")\n",
        "            self.lib = None\n",
        "\n",
        "    def __call__(self, nodes: List[TreeNodeV1]) -> torch.Tensor:\n",
        "        try:\n",
        "            if not nodes:\n",
        "                logger.debug(\"AttentionAggregatorWrapper called with empty node list.\")\n",
        "                return torch.zeros(self.dim, device=self.device)\n",
        "\n",
        "            vectors = []\n",
        "            for i, node in enumerate(nodes):\n",
        "                if isinstance(node, TreeNodeV1):\n",
        "                    vec = node.get_vector()\n",
        "                    if vec is None:\n",
        "                        logger.debug(f\"Node[{i}] ({getattr(node,'id',i)}) has no vector — skipping.\")\n",
        "                        continue\n",
        "                    vec = vec.detach().cpu().to(dtype=torch.float32).reshape(-1)\n",
        "                    if vec.numel() < 1:\n",
        "                        continue\n",
        "                    if torch.isnan(vec).any() or torch.isinf(vec).any():\n",
        "                        logger.debug(f\"Node[{i}] has NaN/Inf in vector — replacing with zeros for that vector.\")\n",
        "                        vec = torch.nan_to_num(vec, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                    # convert to numpy for C wrapper or python fallback\n",
        "                    vec_np = vec.cpu().numpy()\n",
        "                    # ensure length >= dim by padding/truncating\n",
        "                    if vec_np.size < self.dim:\n",
        "                        pad = np.zeros(self.dim - vec_np.size, dtype=np.float32)\n",
        "                        vec_np = np.concatenate([vec_np, pad], axis=0)\n",
        "                    elif vec_np.size > self.dim:\n",
        "                        vec_np = vec_np[:self.dim]\n",
        "                    vectors.append(vec_np)\n",
        "                else:\n",
        "                    logger.debug(f\"Item[{i}] is not TreeNodeV1 — skipping.\")\n",
        "\n",
        "            if not vectors:\n",
        "                logger.warning(\"No valid vectors after processing nodes; returning zero vector.\")\n",
        "                return torch.zeros(self.dim, device=self.device)\n",
        "\n",
        "            # Use C shared lib if available\n",
        "            if self.lib is not None:\n",
        "                try:\n",
        "                    num_vectors = len(vectors)\n",
        "                    flat = np.ascontiguousarray(np.concatenate(vectors, axis=0).astype(np.float32))\n",
        "                    c_array = (ctypes.c_float * flat.size)(*flat.tolist())\n",
        "                    out_array = (ctypes.c_float * self.dim)()\n",
        "                    self.lib.attention_aggregate(c_array, ctypes.c_int(num_vectors), ctypes.c_int(self.dim), out_array)\n",
        "                    out_np = np.ctypeslib.as_array(out_array)\n",
        "                    out = torch.tensor(out_np, dtype=torch.float32, device=self.device)\n",
        "                    if torch.isnan(out).any() or torch.isinf(out).any():\n",
        "                        logger.warning(\"C aggregator produced NaN/Inf — falling back to Python aggregator.\")\n",
        "                        return _python_attention_aggregate(vectors, self.dim).to(self.device)\n",
        "                    return out\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"C aggregator failed at runtime ({e}) — falling back to Python aggregator.\")\n",
        "                    return _python_attention_aggregate(vectors, self.dim).to(self.device)\n",
        "            else:\n",
        "                # pure-python fallback\n",
        "                return _python_attention_aggregate(vectors, self.dim).to(self.device)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Attention aggregation unexpected error: {e}\")\n",
        "            return torch.zeros(self.dim, device=self.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR1CmuqO8cQC",
        "outputId": "66cca530-f38d-48b5-d166-54ee159f20c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/symbolic_feature_extractor.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/symbolic_feature_extractor.py\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def symbolic_feature_extract_py(bgr_img: np.ndarray, spiral_indices: list, dim: int = 64) -> np.ndarray:\n",
        "    if bgr_img is None or bgr_img.ndim != 3:\n",
        "        return np.zeros(dim, dtype=np.float32)\n",
        "\n",
        "    gray = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)\n",
        "    sobelx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    sobely = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    h, w = gray.shape\n",
        "\n",
        "    vec = np.zeros(dim, dtype=np.float32)\n",
        "    for i, idx in enumerate(spiral_indices[:dim]):\n",
        "        y, x = divmod(idx, w)\n",
        "        if 0 <= y < h and 0 <= x < w:\n",
        "            intensity = gray[y, x] / 255.0\n",
        "            gx, gy = sobelx[y, x], sobely[y, x]\n",
        "            vec[i] = intensity + 0.5 * (abs(gx) + abs(gy)) / 255.0\n",
        "        else:\n",
        "            vec[i] = 0.0\n",
        "    return vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGhu9TSFjPWx",
        "outputId": "b8534723-ab45-4c1e-c0ac-11217d66644c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/neural_enhancer.c\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/neural_enhancer.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "#ifdef __cplusplus\n",
        "extern \"C\" {\n",
        "#endif\n",
        "\n",
        "void neural_enhance(float* input, int dim, float* output) {\n",
        "    if (!input || !output || dim <= 0) return;\n",
        "    for (int i = 0; i < dim; i++) {\n",
        "        output[i] = tanh(input[i] * 2.0);\n",
        "    }\n",
        "}\n",
        "\n",
        "#ifdef __cplusplus\n",
        "}\n",
        "#endif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL6-8uQw9oV9",
        "outputId": "2ecf0cb9-4f3d-4b5e-9a76-7c26a442201d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/neural_enhancer_wrapper.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/neural_enhancer_wrapper.py\n",
        "import ctypes\n",
        "import torch\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"neural_enhancer_wrapper\")\n",
        "\n",
        "\n",
        "def _python_neural_enhance(vec: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Simple numerical stable nonlinear enhancement fallback:\n",
        "      out = tanh(2.0 * vec) (same idea as C), applied elementwise with NaN/Inf protection.\n",
        "    \"\"\"\n",
        "    if vec is None:\n",
        "        return None\n",
        "    v = vec.detach().cpu().float()\n",
        "    v = torch.nan_to_num(v, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return torch.tanh(v * 2.0)\n",
        "\n",
        "\n",
        "class NeuralEnhancerWrapper:\n",
        "    def __init__(self, dim: int = 64, device: str = 'cpu'):\n",
        "        self.dim = dim\n",
        "        self.device = device\n",
        "        self.lib = None\n",
        "        self._load_shared_lib()\n",
        "\n",
        "    def _load_shared_lib(self):\n",
        "        try:\n",
        "            self.lib = ctypes.CDLL('./libneural_enhancer.so')\n",
        "            self.lib.neural_enhance.argtypes = [\n",
        "                ctypes.POINTER(ctypes.c_float), ctypes.c_int,\n",
        "                ctypes.POINTER(ctypes.c_float)\n",
        "            ]\n",
        "            self.lib.neural_enhance.restype = None\n",
        "            logger.info(\"Loaded libneural_enhancer.so successfully.\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not load libneural_enhancer.so — using Python fallback. ({e})\")\n",
        "            self.lib = None\n",
        "\n",
        "    def __call__(self, vec: torch.Tensor) -> torch.Tensor:\n",
        "        try:\n",
        "            if vec is None:\n",
        "                logger.debug(\"NeuralEnhancerWrapper called with None vector.\")\n",
        "                return torch.zeros(self.dim, device=self.device)\n",
        "            if not isinstance(vec, torch.Tensor):\n",
        "                vec = torch.tensor(vec, dtype=torch.float32)\n",
        "            v = vec.detach().cpu().float().reshape(-1)\n",
        "            # pad/truncate\n",
        "            if v.numel() < self.dim:\n",
        "                v = torch.cat([v, torch.zeros(self.dim - v.numel(), dtype=torch.float32)], dim=0)\n",
        "            elif v.numel() > self.dim:\n",
        "                v = v[:self.dim]\n",
        "\n",
        "            # try C lib if available\n",
        "            if self.lib is not None:\n",
        "                try:\n",
        "                    in_np = v.numpy().astype(np.float32)\n",
        "                    in_c = (ctypes.c_float * self.dim)(*in_np.tolist())\n",
        "                    out_c = (ctypes.c_float * self.dim)()\n",
        "                    self.lib.neural_enhance(in_c, ctypes.c_int(self.dim), out_c)\n",
        "                    out_np = np.ctypeslib.as_array(out_c)\n",
        "                    out = torch.tensor(out_np, dtype=torch.float32, device=self.device)\n",
        "                    if torch.isnan(out).any() or torch.isinf(out).any():\n",
        "                        logger.warning(\"C neural enhancer produced NaN/Inf — falling back to Python.\")\n",
        "                        return _python_neural_enhance(v).to(self.device)\n",
        "                    return out\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"C neural enhancer failed ({e}) — using Python fallback.\")\n",
        "                    return _python_neural_enhance(v).to(self.device)\n",
        "            else:\n",
        "                return _python_neural_enhance(v).to(self.device)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Neural enhancement unexpected error: {e}\")\n",
        "            return torch.zeros(self.dim, device=self.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klNR11kzBuDJ",
        "outputId": "f6f50069-dcc1-4556-d959-5f5ecc791ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/multimodal_fusion_weights.c\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/multimodal_fusion_weights.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "#ifdef __cplusplus\n",
        "extern \"C\" {\n",
        "#endif\n",
        "\n",
        "void compute_fusion_weights(float* symbolic, float* neural, int dim, float* weights) {\n",
        "    if (!symbolic || !neural || !weights || dim <= 0) return;\n",
        "    float symbolic_norm = 0.0, neural_norm = 0.0;\n",
        "    for (int i = 0; i < dim; i++) {\n",
        "        symbolic_norm += symbolic[i] * symbolic[i];\n",
        "        neural_norm += neural[i] * neural[i];\n",
        "    }\n",
        "    symbolic_norm = sqrt(symbolic_norm);\n",
        "    neural_norm = sqrt(neural_norm);\n",
        "    float total = symbolic_norm + neural_norm;\n",
        "    weights[0] = total > 0 ? symbolic_norm / total : 0.5;\n",
        "    weights[1] = total > 0 ? neural_norm / total : 0.5;\n",
        "}\n",
        "\n",
        "#ifdef __cplusplus\n",
        "}\n",
        "#endif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMpGz-LMBuL2",
        "outputId": "4a62fe2b-29c5-43c8-be65-a042d9e46392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/multimodal_fusion_weights_wrapper.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/multimodal_fusion_weights_wrapper.py\n",
        "import ctypes\n",
        "import torch\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"multimodal_fusion_weights_wrapper\")\n",
        "\n",
        "\n",
        "def _python_compute_fusion_weights(symbolic_vec: torch.Tensor, neural_vec: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Fallback strategy:\n",
        "      - compute L2 norms and normalize: [norm_sym / (norm_sym+norm_neu), norm_neu / (...)]\n",
        "      - stable to zero norms\n",
        "    \"\"\"\n",
        "    def safe_norm(x):\n",
        "        if x is None:\n",
        "            return 0.0\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = torch.tensor(x, dtype=torch.float32)\n",
        "        x = x.detach().cpu().float()\n",
        "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        return float(torch.norm(x).item())\n",
        "\n",
        "    ns = safe_norm(symbolic_vec)\n",
        "    nn = safe_norm(neural_vec)\n",
        "    total = ns + nn\n",
        "    if total <= 1e-12:\n",
        "        return torch.tensor([0.5, 0.5], dtype=torch.float32)\n",
        "    return torch.tensor([ns / total, nn / total], dtype=torch.float32)\n",
        "\n",
        "\n",
        "class MultimodalFusionWeightsWrapper:\n",
        "    def __init__(self, dim: int = 64, device: str = 'cpu'):\n",
        "        self.dim = dim\n",
        "        self.device = device\n",
        "        self.lib = None\n",
        "        self._load_shared_lib()\n",
        "\n",
        "    def _load_shared_lib(self):\n",
        "        try:\n",
        "            self.lib = ctypes.CDLL('./multimodal_fusion_weights.so')\n",
        "            self.lib.compute_fusion_weights.argtypes = [\n",
        "                ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float),\n",
        "                ctypes.c_int, ctypes.POINTER(ctypes.c_float)\n",
        "            ]\n",
        "            self.lib.compute_fusion_weights.restype = None\n",
        "            logger.info(\"Loaded multimodal_fusion_weights.so successfully.\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not load multimodal_fusion_weights.so — using Python fallback. ({e})\")\n",
        "            self.lib = None\n",
        "\n",
        "    def compute_weights(self, symbolic_vec: torch.Tensor, neural_vec: torch.Tensor) -> torch.Tensor:\n",
        "        try:\n",
        "            if symbolic_vec is None or neural_vec is None:\n",
        "                logger.warning(\"One or both input vectors are None — returning default [0.5,0.5].\")\n",
        "                return torch.tensor([0.5, 0.5], dtype=torch.float32, device=self.device)\n",
        "\n",
        "            # prepare fixed-size arrays\n",
        "            s = torch.tensor(symbolic_vec, dtype=torch.float32).detach().cpu().reshape(-1)\n",
        "            n = torch.tensor(neural_vec, dtype=torch.float32).detach().cpu().reshape(-1)\n",
        "            if s.numel() < self.dim:\n",
        "                s = torch.cat([s, torch.zeros(self.dim - s.numel(), dtype=torch.float32)], dim=0)\n",
        "            else:\n",
        "                s = s[:self.dim]\n",
        "            if n.numel() < self.dim:\n",
        "                n = torch.cat([n, torch.zeros(self.dim - n.numel(), dtype=torch.float32)], dim=0)\n",
        "            else:\n",
        "                n = n[:self.dim]\n",
        "\n",
        "            if self.lib is not None:\n",
        "                try:\n",
        "                    s_arr = (ctypes.c_float * self.dim)(*s.numpy().tolist())\n",
        "                    n_arr = (ctypes.c_float * self.dim)(*n.numpy().tolist())\n",
        "                    out_arr = (ctypes.c_float * 2)()\n",
        "                    self.lib.compute_fusion_weights(s_arr, n_arr, ctypes.c_int(self.dim), out_arr)\n",
        "                    w0 = float(out_arr[0])\n",
        "                    w1 = float(out_arr[1])\n",
        "                    out = torch.tensor([w0, w1], dtype=torch.float32, device=self.device)\n",
        "                    # sanitize\n",
        "                    out = torch.nan_to_num(out, nan=0.5, posinf=0.5, neginf=0.5)\n",
        "                    ssum = float(out.sum())\n",
        "                    if abs(ssum) < 1e-8:\n",
        "                        return torch.tensor([0.5, 0.5], dtype=torch.float32, device=self.device)\n",
        "                    return (out / ssum).to(self.device)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"C compute_fusion_weights failed ({e}) — using Python fallback.\")\n",
        "                    return _python_compute_fusion_weights(s, n).to(self.device)\n",
        "            else:\n",
        "                return _python_compute_fusion_weights(s, n).to(self.device)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Fusion weights unexpected error: {e}\")\n",
        "            return torch.tensor([0.5, 0.5], dtype=torch.float32, device=self.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM2dgfZlBlY4",
        "outputId": "2b9a2b9f-2771-459f-a96f-8c0e9082456e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/shape_abstraction.c\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/shape_abstraction.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "#ifdef __cplusplus\n",
        "extern \"C\" {\n",
        "#endif\n",
        "\n",
        "void shape_embed(float* input, int dim, float* output) {\n",
        "    if (!input || !output || dim <= 0) return;\n",
        "    float norm = 0.0;\n",
        "    for (int i = 0; i < dim; i++) {\n",
        "        norm += input[i] * input[i];\n",
        "    }\n",
        "    norm = sqrt(norm) > 0 ? sqrt(norm) : 1.0;\n",
        "    for (int i = 0; i < dim; i++) {\n",
        "        output[i] = input[i] / norm;\n",
        "    }\n",
        "}\n",
        "\n",
        "#ifdef __cplusplus\n",
        "}\n",
        "#endif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfFdNw5CBlug",
        "outputId": "4e11704b-ca9b-4d06-a3d5-460efaba48d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/shape_abstraction_wrapper.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/shape_abstraction_wrapper.py\n",
        "import ctypes\n",
        "import torch\n",
        "import logging\n",
        "import numpy as np\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"shape_abstraction_wrapper\")\n",
        "\n",
        "\n",
        "def _python_normalize(vec: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Safe fallback normalization.\"\"\"\n",
        "    v = vec.float()\n",
        "    v = torch.nan_to_num(v, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    n = torch.norm(v).item()\n",
        "    if n <= 1e-8:\n",
        "        return torch.zeros_like(v)\n",
        "    return v / n\n",
        "\n",
        "\n",
        "class ShapeAbstractionWrapper:\n",
        "    def __init__(self, dim=64, device='cpu'):\n",
        "        self.device = device\n",
        "        self.dim = dim\n",
        "        self.lib = None\n",
        "        self._load()\n",
        "\n",
        "    def _load(self):\n",
        "        try:\n",
        "            self.lib = ctypes.CDLL('./shape_abstraction.so')\n",
        "            self.lib.shape_embed.argtypes = [\n",
        "                ctypes.POINTER(ctypes.c_float),\n",
        "                ctypes.c_int,\n",
        "                ctypes.POINTER(ctypes.c_float)\n",
        "            ]\n",
        "            self.lib.shape_embed.restype = None\n",
        "            logger.info(\"Loaded shape_abstraction.so successfully.\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not load shape_abstraction.so — using Python fallback. ({e})\")\n",
        "            self.lib = None\n",
        "\n",
        "    def __call__(self, node: TreeNodeV1) -> torch.Tensor:\n",
        "        try:\n",
        "            vec = node.get_vector()\n",
        "            if vec is None:\n",
        "                return torch.zeros(self.dim, device=self.device)\n",
        "\n",
        "            v = vec.detach().cpu().float().view(-1)\n",
        "\n",
        "            # pad/truncate to dim\n",
        "            if v.numel() < self.dim:\n",
        "                v = torch.cat([v, torch.zeros(self.dim - v.numel())])\n",
        "            elif v.numel() > self.dim:\n",
        "                v = v[:self.dim]\n",
        "\n",
        "            # Use C library if available\n",
        "            if self.lib:\n",
        "                try:\n",
        "                    in_arr = (ctypes.c_float * self.dim)(*v.numpy().tolist())\n",
        "                    out_arr = (ctypes.c_float * self.dim)()\n",
        "                    self.lib.shape_embed(in_arr, self.dim, out_arr)\n",
        "                    out = torch.tensor(list(out_arr), dtype=torch.float32)\n",
        "                    out = torch.nan_to_num(out)\n",
        "                    return out.to(self.device)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"C library failed, using python fallback ({e})\")\n",
        "                    return _python_normalize(v).to(self.device)\n",
        "\n",
        "            # python fallback\n",
        "            return _python_normalize(v).to(self.device)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Shape abstraction failure: {e}\")\n",
        "            return torch.zeros(self.dim, device=self.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jB0pVnMpEs0",
        "outputId": "89bd35e0-5a8a-49e9-bc9b-4221b7b5c293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/adaptive_depth.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/adaptive_depth.py\n",
        "import logging\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"adaptive_depth\")\n",
        "\n",
        "\n",
        "class AdaptiveDepthController:\n",
        "    def __init__(self, max_depth=4, min_confidence=0.2, min_feature_confidence=0.1):\n",
        "        \"\"\"\n",
        "        Fully shape-independent depth controller.\n",
        "        \"\"\"\n",
        "        self.max_depth = int(max_depth)\n",
        "        self.min_confidence = float(min_confidence)\n",
        "        self.min_feature_confidence = float(min_feature_confidence)\n",
        "\n",
        "    def control_depth(self, node: TreeNodeV1, depth: int = 0):\n",
        "        try:\n",
        "            if not isinstance(node, TreeNodeV1):\n",
        "                return None\n",
        "\n",
        "            if depth >= self.max_depth:\n",
        "                return None\n",
        "\n",
        "            if node.get_confidence() < self.min_confidence:\n",
        "                return None\n",
        "\n",
        "            # prune subfeatures\n",
        "            for k, conf in list(node.feature_confidence.items()):\n",
        "                if conf < self.min_feature_confidence:\n",
        "                    node.sub_features.pop(k, None)\n",
        "                    node.feature_confidence.pop(k, None)\n",
        "\n",
        "            # recursive prune\n",
        "            children = []\n",
        "            for child in node.children:\n",
        "                pruned = self.control_depth(child, depth + 1)\n",
        "                if pruned:\n",
        "                    children.append(pruned)\n",
        "            node.children = children\n",
        "\n",
        "            return node if node.get_vector() is not None else None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Depth control failed: {e}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B03hkorGy9w5",
        "outputId": "e1fb8ae8-5778-4112-c541-097d1af7d6cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/sparse_tree_optimizer.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/sparse_tree_optimizer.py\n",
        "import torch\n",
        "import logging\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"sparse_tree_optimizer\")\n",
        "\n",
        "\n",
        "class SparseTreeOptimizer:\n",
        "    def __init__(self, threshold=0.1):\n",
        "        \"\"\"\n",
        "        Shape-independent sparsity optimizer.\n",
        "        threshold = |x| > threshold keeps values; else zero.\n",
        "        \"\"\"\n",
        "        self.threshold = float(threshold)\n",
        "\n",
        "    def optimize(self, node: TreeNodeV1):\n",
        "        try:\n",
        "            if node is None:\n",
        "                return None\n",
        "\n",
        "            vec = node.get_vector()\n",
        "            if vec is not None:\n",
        "                mask = (vec.abs() > self.threshold).float()\n",
        "                node.store_vector(vec * mask)\n",
        "\n",
        "            children = []\n",
        "            for child in node.children:\n",
        "                opt = self.optimize(child)\n",
        "                if opt:\n",
        "                    children.append(opt)\n",
        "            node.children = children\n",
        "\n",
        "            return node if node.get_vector() is not None or node.children else None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Optimize error: {e}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCz6nHrPy-gA",
        "outputId": "c544b963-e0a6-4f48-bbf1-55a6730b050c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tree_matrix_hybrid_wrapper.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/tree_matrix_hybrid_wrapper.py\n",
        "import ctypes\n",
        "import torch\n",
        "import logging\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"tree_matrix_hybrid\")\n",
        "\n",
        "\n",
        "class TreeMatrixHybridWrapper:\n",
        "    def __init__(self, dim=64, device='cpu'):\n",
        "        self.dim = dim\n",
        "        self.device = device\n",
        "        self.lib = None\n",
        "        self._load()\n",
        "\n",
        "    def _load(self):\n",
        "        try:\n",
        "            self.lib = ctypes.CDLL('./tree_matrix_hybrid.so')\n",
        "            self.lib.tree_matrix_hybrid.argtypes = [\n",
        "                ctypes.POINTER(ctypes.c_float), ctypes.c_int, ctypes.c_int,\n",
        "                ctypes.POINTER(ctypes.c_int), ctypes.c_int\n",
        "            ]\n",
        "            self.lib.tree_matrix_hybrid.restype = ctypes.c_void_p\n",
        "            self.lib.free_matrix.argtypes = [ctypes.c_void_p]\n",
        "            logger.info(\"Loaded tree_matrix_hybrid.so\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not load tree_matrix_hybrid.so ({e})\")\n",
        "            self.lib = None\n",
        "\n",
        "    def to_matrix(self, root: TreeNodeV1):\n",
        "        try:\n",
        "            if root is None:\n",
        "                return torch.zeros((1, self.dim), device=self.device)\n",
        "\n",
        "            # collect vectors\n",
        "            data = []\n",
        "            idxs = []\n",
        "\n",
        "            def walk(n):\n",
        "                v = n.get_vector()\n",
        "                if v is not None:\n",
        "                    vec = v.detach().cpu().float().view(-1)\n",
        "                    if vec.numel() < self.dim:\n",
        "                        vec = torch.cat([vec, torch.zeros(self.dim - vec.numel())])\n",
        "                    elif vec.numel() > self.dim:\n",
        "                        vec = vec[:self.dim]\n",
        "                    data.extend(vec.tolist())\n",
        "                    idxs.append(len(idxs))\n",
        "                for c in n.children:\n",
        "                    walk(c)\n",
        "\n",
        "            walk(root)\n",
        "\n",
        "            node_count = len(idxs)\n",
        "            if node_count == 0:\n",
        "                return torch.zeros((1, self.dim), device=self.device)\n",
        "\n",
        "            if self.lib is None:\n",
        "                # Python fallback\n",
        "                mat = torch.tensor(data, dtype=torch.float32).view(node_count, self.dim)\n",
        "                return mat.to(self.device)\n",
        "\n",
        "            # C call\n",
        "            data_arr = (ctypes.c_float * len(data))(*data)\n",
        "            idx_arr = (ctypes.c_int * len(idxs))(*idxs)\n",
        "            ptr = self.lib.tree_matrix_hybrid(data_arr, node_count, self.dim,\n",
        "                                              idx_arr, len(idxs))\n",
        "\n",
        "            if not ptr:\n",
        "                logger.warning(\"tree_matrix_hybrid returned NULL\")\n",
        "                return torch.zeros((node_count, self.dim), device=self.device)\n",
        "\n",
        "            mat_flat = ctypes.cast(ptr, ctypes.POINTER(ctypes.c_float *\n",
        "                                                       (node_count * self.dim))).contents\n",
        "            mat = torch.tensor(list(mat_flat), dtype=torch.float32).reshape(node_count, self.dim)\n",
        "            self.lib.free_matrix(ptr)\n",
        "            return mat.to(self.device)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Matrix hybrid error: {e}\")\n",
        "            return torch.zeros((1, self.dim), device=self.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN6PRgD02EYm"
      },
      "outputs": [],
      "source": [
        "\n",
        "!apt-get update\n",
        "!apt-get install -y libopencv-dev\n",
        "!g++ -v /content/lazy_recursive.c -o /content/lazy_recursive.so -shared -fPIC\n",
        "!g++ -v /content/tree_pruner.c -o /content/tree_pruner.so -shared -fPIC\n",
        "!g++ -v /content/quantization.c -o /content/quantization.so -shared -fPIC\n",
        "!g++ -v /content/tree_matrix_hybrid.c -o /content/tree_matrix_hybrid.so -shared -fPIC\n",
        "!g++ -v /content/symbolic_feature_extractor.c -o /content/symbolic_feature_extractor.so -lopencv_core -lopencv_imgproc -shared -fPIC\n",
        "!g++ -v /content/neural_enhancer.c -o /content/neural_enhancer.so -shared -fPIC\n",
        "!g++ -v /content/attention_aggregator.c -o /content/attention_aggregator.so -shared -fPIC\n",
        "!g++ -v /content/shape_abstraction.c -o /content/shape_abstraction.so -shared -fPIC\n",
        "!g++ -v /content/multimodal_fusion_weights.c -o /content/multimodal_fusion_weights.so -shared -fPIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3bMSkzDOlDl"
      },
      "outputs": [],
      "source": [
        "!mv neural_enhancer.so libneural_enhancer.so"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUOKR_RR1k09",
        "outputId": "eb9baf05-8b8a-4386-e746-a6c2cb08e2a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tree_fusion_engine.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile /content/tree_fusion_engine.py\n",
        "import logging\n",
        "import torch\n",
        "\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "from adaptive_depth import AdaptiveDepthController\n",
        "from sparse_tree_optimizer import SparseTreeOptimizer\n",
        "from attention_aggregator_wrapper import AttentionAggregatorWrapper\n",
        "from neural_enhancer_wrapper import NeuralEnhancerWrapper\n",
        "from quantization_wrapper import QuantizationWrapper\n",
        "from multimodal_fusion_weights_wrapper import MultimodalFusionWeightsWrapper\n",
        "from tree_matrix_hybrid_wrapper import TreeMatrixHybridWrapper\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class TreeCNNFusionEngine:\n",
        "    def __init__(self, dim=64, device='cpu', efficiency_flag=\"--efficiency=high\"):\n",
        "        self.device = device\n",
        "        self.dim = dim\n",
        "        self.efficiency_flag = efficiency_flag\n",
        "\n",
        "        # shape-independent modules\n",
        "        self.depth_controller = AdaptiveDepthController(\n",
        "        max_depth=4,\n",
        "        min_confidence=0.0,\n",
        "        min_feature_confidence=0.0\n",
        "        )\n",
        "        self.sparse_optimizer = SparseTreeOptimizer(threshold=0.1)\n",
        "        self.quantizer = QuantizationWrapper(dim=dim)\n",
        "        self.attn_aggregator = AttentionAggregatorWrapper(dim=dim, device=device)\n",
        "        self.enhancer = NeuralEnhancerWrapper(dim=dim)\n",
        "        self.weights = MultimodalFusionWeightsWrapper(dim=dim, device=device)\n",
        "        self.matrix_converter = TreeMatrixHybridWrapper(dim, device=device)\n",
        "\n",
        "    def process(self, root: TreeNodeV1) -> torch.Tensor:\n",
        "        try:\n",
        "            root = self.depth_controller.control_depth(root)\n",
        "            if root is None:\n",
        "                return torch.zeros(self.dim, device=self.device)\n",
        "\n",
        "            root = self.sparse_optimizer.optimize(root)\n",
        "            self.quantizer.apply_quantization(root, user_flag=self.efficiency_flag)\n",
        "\n",
        "            valid = [c for c in root.children if c.get_vector() is not None]\n",
        "            agg = self.attn_aggregator(valid)\n",
        "\n",
        "            enhanced = self.enhancer(agg)\n",
        "\n",
        "            # fusion: symbolic removed → use neural only\n",
        "            symbolic = torch.zeros(self.dim, device=self.device)\n",
        "            w = self.weights.compute_weights(symbolic, enhanced)\n",
        "\n",
        "            fused = (w[0] * symbolic) + (w[1] * enhanced)\n",
        "            return fused.to(self.device)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"FusionEngine failed: {e}\")\n",
        "            return torch.zeros(self.dim, device=self.device)\n",
        "\n",
        "    def to_matrix(self, root: TreeNodeV1) -> torch.Tensor:\n",
        "        return self.matrix_converter.to_matrix(root)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section"
      ],
      "metadata": {
        "id": "6hLa8ODTxrN6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T17xiWVvFSCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76358a52-58b0-4e5c-b46a-fab418a8e187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/label_pipeline.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/label_pipeline.py\n",
        "from tokenizer_and_embedding import TokenEmbedding, universal_tokenizer\n",
        "from target_processor import MultiLabelTargetProcessor\n",
        "from TreeBuilderV2 import TreeBuilderV2\n",
        "import torch\n",
        "\n",
        "def run_label_pipeline(label_text, dim=50, device=\"cpu\"):\n",
        "    tokens = universal_tokenizer(label_text)\n",
        "    vocab = sorted(set(tokens))\n",
        "    token_embedding = TokenEmbedding(vocab=vocab, dim=dim, device=device)\n",
        "\n",
        "    target_processor = MultiLabelTargetProcessor()\n",
        "    target_processor.fit_labels([label_text])\n",
        "\n",
        "    vec_pairs = []\n",
        "    for i, token in enumerate(tokens):\n",
        "        vec = token_embedding.lookup(token)  # Already returns a torch.Tensor\n",
        "        vec_pairs.append((f\"label_node_{i}\", vec.to(device)))\n",
        "\n",
        "    builder = TreeBuilderV2(device=device, dim=dim)\n",
        "    label_tree = builder.build_tree(\n",
        "        vec_pairs,\n",
        "        label_texts={t: label_text for t, _ in vec_pairs},\n",
        "        token_embedding=token_embedding,\n",
        "        target_processor=target_processor\n",
        "    )\n",
        "\n",
        "    return label_tree"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section"
      ],
      "metadata": {
        "id": "_YovypZKxvrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile /content/treecnn_pipeline.py\n",
        "import torch\n",
        "import os\n",
        "import logging\n",
        "\n",
        "from TreeCNNppRunner import run_treecnnpp\n",
        "from tree_fusion_engine import TreeCNNFusionEngine\n",
        "from label_pipeline import run_label_pipeline\n",
        "\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(\"TreeCNNPipeline\")\n",
        "\n",
        "\n",
        "class TreeCNNPipeline:\n",
        "    \"\"\"\n",
        "    FINAL SAFE + SHAPE-FREE TreeCNN++ PIPELINE\n",
        "    ------------------------------------------\n",
        "    • No TLite or shape tags.\n",
        "    • Fully device-safe (CPU/GPU)\n",
        "    • Guaranteed output dim = self.dim\n",
        "    • Optional on-disk caching for precompute efficiency\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device=\"cpu\", dim=64, cache_dir=\"/content/treecnn_cache\"):\n",
        "        self.device = device\n",
        "        self.dim = dim\n",
        "\n",
        "        # All extracted vectors will be dim-sized\n",
        "        self.shape_dim = dim\n",
        "\n",
        "        # disabled TLite\n",
        "        self.tlite_shape_model = None\n",
        "        self.shape_to_idx = {}\n",
        "\n",
        "        # Fusion engine\n",
        "        self.fusion = TreeCNNFusionEngine(dim=dim, device=device)\n",
        "\n",
        "        # Optional caching\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "\n",
        "        logger.info(f\"TreeCNNPipeline initialized (dim={dim}, device={device})\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # INTERNAL: sanitize final vector\n",
        "    # -------------------------------\n",
        "    def _safe_vec(self, vec, tag=\"unknown\"):\n",
        "        \"\"\"\n",
        "        Ensure vec is a torch.Tensor of size [dim] and contains no NaNs.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if vec is None:\n",
        "                raise ValueError(\"None vector\")\n",
        "\n",
        "            if not isinstance(vec, torch.Tensor):\n",
        "                vec = torch.tensor(vec, dtype=torch.float32)\n",
        "\n",
        "            vec = vec.to(self.device).float()\n",
        "\n",
        "            # Wrong dimension → fix\n",
        "            if vec.numel() != self.dim:\n",
        "                logger.warning(f\"[{tag}] wrong dim {vec.numel()} → forced to {self.dim}\")\n",
        "                v = vec.view(-1)\n",
        "                if v.numel() < self.dim:\n",
        "                    pad = torch.zeros(self.dim - v.numel(), device=self.device)\n",
        "                    v = torch.cat([v, pad], dim=0)\n",
        "                vec = v[:self.dim]\n",
        "\n",
        "            # NaN / Inf guard\n",
        "            if torch.isnan(vec).any() or torch.isinf(vec).any():\n",
        "                logger.error(f\"[{tag}] NaN detected → replaced with zeros\")\n",
        "                return torch.zeros(self.dim, device=self.device)\n",
        "\n",
        "            return vec\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"[{tag}] safe_vec failed: {e}\")\n",
        "            return torch.zeros(self.dim, device=self.device)\n",
        "\n",
        "    # --------------------------------\n",
        "    # OPTIONAL CACHING (FAST TRAINING)\n",
        "    # --------------------------------\n",
        "    def _cache_path(self, key: str):\n",
        "        return os.path.join(self.cache_dir, f\"{key}.pt\")\n",
        "\n",
        "    def _load_cache(self, key: str):\n",
        "        path = self._cache_path(key)\n",
        "        if os.path.exists(path):\n",
        "            try:\n",
        "                return torch.load(path, map_location=self.device)\n",
        "            except:\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "    def _save_cache(self, key: str, vec):\n",
        "        try:\n",
        "            torch.save(vec.detach().cpu(), self._cache_path(key))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # --------------------------------\n",
        "    # IMAGE → fused vector\n",
        "    # --------------------------------\n",
        "    def process_image(self, image_path, use_parallel=True):\n",
        "        cache_key = f\"img::{os.path.basename(image_path)}\"\n",
        "\n",
        "        cached = self._load_cache(cache_key)\n",
        "        if cached is not None:\n",
        "            return self._safe_vec(cached, tag=\"cache_image\")\n",
        "\n",
        "        # Build tree\n",
        "        tree = run_treecnnpp(\n",
        "            image_path=image_path,\n",
        "            device=self.device,\n",
        "            dim=self.shape_dim,\n",
        "            use_parallel=use_parallel\n",
        "        )\n",
        "\n",
        "        if tree is None:\n",
        "            logger.warning(f\"[image] TreeCNN failed: {image_path}\")\n",
        "            vec = torch.zeros(self.dim, device=self.device)\n",
        "            self._save_cache(cache_key, vec)\n",
        "            return vec\n",
        "\n",
        "        fused = self.fusion.process(tree)\n",
        "        fused = self._safe_vec(fused, tag=\"image_fusion\")\n",
        "\n",
        "        self._save_cache(cache_key, fused)\n",
        "        return fused\n",
        "\n",
        "    # --------------------------------\n",
        "    # TEXT → fused vector\n",
        "    # --------------------------------\n",
        "    def process_text(self, text):\n",
        "        cache_key = f\"text::{text.strip().replace(' ','_')[:80]}\"\n",
        "\n",
        "        cached = self._load_cache(cache_key)\n",
        "        if cached is not None:\n",
        "            return self._safe_vec(cached, tag=\"cache_text\")\n",
        "\n",
        "        tree = run_label_pipeline(\n",
        "            text=text,\n",
        "            dim=self.shape_dim,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        if tree is None:\n",
        "            logger.warning(f\"[text] label tree failed: {text}\")\n",
        "            vec = torch.zeros(self.dim, device=self.device)\n",
        "            self._save_cache(cache_key, vec)\n",
        "            return vec\n",
        "\n",
        "        fused = self.fusion.process(tree)\n",
        "        fused = self._safe_vec(fused, tag=\"text_fusion\")\n",
        "\n",
        "        self._save_cache(cache_key, fused)\n",
        "        return fused\n",
        "\n",
        "    # --------------------------------\n",
        "    # GROUP → fused vector\n",
        "    # --------------------------------\n",
        "    def process_group(self, combined_tree):\n",
        "        if combined_tree is None:\n",
        "            return torch.zeros(self.dim, device=self.device)\n",
        "\n",
        "        fused = self.fusion.process(combined_tree)\n",
        "        return self._safe_vec(fused, tag=\"group_fusion\")"
      ],
      "metadata": {
        "id": "GCihLyWduFMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffcd8e73-896e-4c05-ae60-525f80508f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/treecnn_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section"
      ],
      "metadata": {
        "id": "5HfOu8jgx5gX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile /content/group_tree_builder.py\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "from TreeNodeV1 import TreeNodeV1\n",
        "from TreeCNNppRunner import run_treecnnpp\n",
        "from label_pipeline import run_label_pipeline\n",
        "\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"GroupTreeBuilder\")\n",
        "\n",
        "\n",
        "class GroupTreeBuilder:\n",
        "    def __init__(self, device: str = 'cpu', dim: int = 50, max_workers: int = 8):\n",
        "        self.device = device\n",
        "        self.dim = dim\n",
        "        self.max_workers = max_workers\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # IMAGE TREE\n",
        "    # ----------------------------------------------------------\n",
        "    def build_image_tree(self, image_path: str) -> Optional[TreeNodeV1]:\n",
        "        try:\n",
        "            tree = run_treecnnpp(\n",
        "                image_path=image_path,\n",
        "                device=self.device,\n",
        "                dim=self.dim,\n",
        "                use_parallel=True\n",
        "            )\n",
        "            if tree is None:\n",
        "                logger.warning(f\"[image] run_treecnnpp returned None for {image_path}\")\n",
        "            return tree\n",
        "        except Exception as e:\n",
        "            logger.error(f\"[image] Failed {image_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # LABEL TREE\n",
        "    # ----------------------------------------------------------\n",
        "    def build_label_tree(self, text: str) -> Optional[TreeNodeV1]:\n",
        "       try:\n",
        "           tree = run_label_pipeline(\n",
        "               label_text=text,      # <- IMPORTANT FIX\n",
        "               dim=self.dim,\n",
        "               device=self.device\n",
        "           )\n",
        "           if tree is None:\n",
        "               logger.warning(f\"[label] empty label tree for text: {text}\")\n",
        "           return tree\n",
        "       except Exception as e:\n",
        "           logger.error(f\"[label] Failed text='{text}': {e}\")\n",
        "           return None\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # COMBINE IMAGE + METADATA + LABEL TREES\n",
        "    # ----------------------------------------------------------\n",
        "    def combine_trees(\n",
        "        self,\n",
        "        image_tree: Optional[TreeNodeV1],\n",
        "        label_trees: List[Optional[TreeNodeV1]],\n",
        "        organ: Optional[str] = None\n",
        "    ) -> TreeNodeV1:\n",
        "\n",
        "        # Generous max_children, but safe\n",
        "        root = TreeNodeV1(\n",
        "            value=\"group_root\",\n",
        "            shape_type=None,\n",
        "            id=\"group_root\",\n",
        "            max_children=5000\n",
        "        )\n",
        "\n",
        "        # 1. add image tree\n",
        "        if isinstance(image_tree, TreeNodeV1):\n",
        "            root.add_child(image_tree)\n",
        "        else:\n",
        "            logger.warning(\"[combine] Missing image tree\")\n",
        "\n",
        "        # 2. add organ metadata (minimal node)\n",
        "        if organ:\n",
        "            organ_node = TreeNodeV1(\n",
        "                value=f\"organ:{organ}\",\n",
        "                shape_type=None,\n",
        "                id=f\"organ:{organ}\",\n",
        "                max_children=0\n",
        "            )\n",
        "            root.add_child(organ_node)\n",
        "\n",
        "        # 3. add label trees\n",
        "        for lt in label_trees:\n",
        "            if isinstance(lt, TreeNodeV1):\n",
        "                root.add_child(lt)\n",
        "\n",
        "        return root\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # MAIN ENTRY: FULL GROUP BUILDER\n",
        "    # ----------------------------------------------------------\n",
        "    def build_combined_for_group(\n",
        "        self,\n",
        "        image_path: str,\n",
        "        questions: List[str],\n",
        "        organ: Optional[str] = None\n",
        "    ) -> Optional[TreeNodeV1]:\n",
        "\n",
        "        # IMAGE NODE\n",
        "        image_tree = self.build_image_tree(image_path)\n",
        "\n",
        "        # QUESTION TEXT NODES (PARALLEL)\n",
        "        label_trees = []\n",
        "        if questions:\n",
        "            with ThreadPoolExecutor(\n",
        "                max_workers=min(self.max_workers, max(1, len(questions)))\n",
        "            ) as ex:\n",
        "                futures = {ex.submit(self.build_label_tree, q): q for q in questions}\n",
        "                for fut in as_completed(futures):\n",
        "                    try:\n",
        "                        lt = fut.result()\n",
        "                        if lt is not None:\n",
        "                            label_trees.append(lt)\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"[label_future] {e}\")\n",
        "\n",
        "        # FINAL GROUP TREE\n",
        "        return self.combine_trees(image_tree, label_trees, organ)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wL8mXap4BCu",
        "outputId": "f7af3116-258a-43dc-c18e-96921356c57c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/group_tree_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile /content/dataset_group_loader.py\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "\n",
        "def normalize_answer(a):\n",
        "    if not isinstance(a, str):\n",
        "        return \"other\"\n",
        "    a = a.lower().strip()\n",
        "\n",
        "    # yes/no group\n",
        "    if a in {\"yes\", \"y\", \"true\", \"1\"}:\n",
        "        return \"yes\"\n",
        "    if a in {\"no\", \"n\", \"false\", \"0\"}:\n",
        "        return \"no\"\n",
        "\n",
        "    # normal\n",
        "    if \"normal\" in a or \"unremarkable\" in a:\n",
        "        return \"normal\"\n",
        "\n",
        "    # opacity / opacification\n",
        "    if \"opacity\" in a or \"opac\" in a:\n",
        "        return \"opacity\"\n",
        "\n",
        "    # enlarged\n",
        "    if \"enlarg\" in a:\n",
        "        return \"enlarged\"\n",
        "\n",
        "    # fracture\n",
        "    if \"fracture\" in a or \"fx\" in a:\n",
        "        return \"fracture\"\n",
        "\n",
        "    # abnormal group keywords\n",
        "    keywords = [\"effusion\", \"pneumo\", \"consolid\", \"lesion\", \"mass\", \"nodule\"]\n",
        "    if any(k in a for k in keywords):\n",
        "        return \"abnormal\"\n",
        "\n",
        "    return \"other\"\n",
        "\n",
        "\n",
        "class GroupedDatasetLoader:\n",
        "    def __init__(\n",
        "        self,\n",
        "        csv_path: str,\n",
        "        image_root: str = None,\n",
        "        image_column: str = \"IMAGEID\",\n",
        "        question_column: str = \"QUESTION\",\n",
        "        answer_column: str = \"ANSWER\",\n",
        "        organ_column: str = \"IMAGEORGAN\",\n",
        "        min_answer_freq: int = 3       # <-- NEW: filter rare answers\n",
        "    ):\n",
        "        self.csv_path = csv_path\n",
        "        self.image_root = image_root\n",
        "        self.image_column = image_column\n",
        "        self.question_column = question_column\n",
        "        self.answer_column = answer_column\n",
        "        self.organ_column = organ_column\n",
        "        self.min_answer_freq = min_answer_freq\n",
        "\n",
        "    def _extract_filename(self, image_val: str) -> str:\n",
        "        if not isinstance(image_val, str):\n",
        "            return str(image_val)\n",
        "        if image_val.startswith(\"http://\") or image_val.startswith(\"https://\"):\n",
        "            return os.path.basename(image_val.split(\"?\")[0])\n",
        "        return os.path.basename(image_val)\n",
        "\n",
        "    def load_and_group(self) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:\n",
        "        df = pd.read_excel(self.csv_path)\n",
        "\n",
        "        groups = defaultdict(lambda: {\n",
        "            \"questions\": [],\n",
        "            \"answers\": [],\n",
        "            \"organs\": [],\n",
        "            \"image_vals\": []\n",
        "        })\n",
        "\n",
        "        # ------------------------------------\n",
        "        # FIRST PASS → Load raw groups\n",
        "        # ------------------------------------\n",
        "        all_answers_raw = []\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            img_val = row.get(self.image_column, None)\n",
        "            if pd.isna(img_val):\n",
        "                continue\n",
        "\n",
        "            q = row.get(self.question_column, \"\")\n",
        "            a = row.get(self.answer_column, \"\")\n",
        "            organ = row.get(self.organ_column, \"\")\n",
        "\n",
        "            key = str(img_val).strip()\n",
        "\n",
        "            groups[key][\"questions\"].append(\"\" if pd.isna(q) else str(q).strip())\n",
        "\n",
        "            # normalize answer\n",
        "            a_norm = normalize_answer(\"\" if pd.isna(a) else str(a))\n",
        "            groups[key][\"answers\"].append(a_norm)\n",
        "            all_answers_raw.append(a_norm)\n",
        "\n",
        "            groups[key][\"organs\"].append(\"\" if pd.isna(organ) else str(organ).strip())\n",
        "            groups[key][\"image_vals\"].append(img_val)\n",
        "\n",
        "        # ------------------------------------\n",
        "        # SECOND PASS → Filter Rare Answers\n",
        "        # ------------------------------------\n",
        "        freq = Counter(all_answers_raw)\n",
        "        allowed = {a for a, c in freq.items() if c >= self.min_answer_freq}\n",
        "\n",
        "        # anything below threshold becomes \"other\"\n",
        "        def map_answer(a):\n",
        "            return a if a in allowed and a != \"\" else \"other\"\n",
        "\n",
        "        # all answers include \"other\"\n",
        "        allowed.add(\"other\")\n",
        "\n",
        "        # ------------------------------------\n",
        "        # THIRD PASS → Build groups + label map\n",
        "        # ------------------------------------\n",
        "        group_list = []\n",
        "        used_answers = set()\n",
        "\n",
        "        for key, val in groups.items():\n",
        "            raw_img_val = val[\"image_vals\"][0]\n",
        "            filename = self._extract_filename(raw_img_val)\n",
        "\n",
        "            if self.image_root:\n",
        "                image_path = os.path.join(self.image_root, filename)\n",
        "            else:\n",
        "                image_path = raw_img_val\n",
        "\n",
        "            mapped_answers = [map_answer(a) for a in val[\"answers\"]]\n",
        "\n",
        "            for a in mapped_answers:\n",
        "                used_answers.add(a)\n",
        "\n",
        "            group_list.append({\n",
        "                \"image_id\": key,\n",
        "                \"image_name\": filename,\n",
        "                \"image_path\": image_path,\n",
        "                \"questions\": val[\"questions\"],\n",
        "                \"answers\": mapped_answers,\n",
        "                \"organ\": val[\"organs\"][0] if val[\"organs\"] else None\n",
        "            })\n",
        "\n",
        "        sorted_answers = sorted(list(used_answers))\n",
        "        label_map = {lab: idx for idx, lab in enumerate(sorted_answers)}\n",
        "\n",
        "        print(f\"⚡ Total unique answers after normalization = {len(label_map)}\")\n",
        "\n",
        "        return group_list, label_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdBF8Ez34Ll-",
        "outputId": "316de610-fad8-40d0-a2f5-538e98e3a639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/dataset_group_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section"
      ],
      "metadata": {
        "id": "w7bGIE6YyKI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile /content/trainer_pipeline.py\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "from dataset_group_loader import GroupedDatasetLoader\n",
        "from group_tree_builder import GroupTreeBuilder\n",
        "from treecnn_pipeline import TreeCNNPipeline\n",
        "\n",
        "# Optional metrics backend: prefer sklearn if available, else fallback\n",
        "try:\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "    _HAVE_SKLEARN = True\n",
        "except Exception:\n",
        "    _HAVE_SKLEARN = False\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Group Dataset (group-level samples)\n",
        "# -------------------------\n",
        "class GroupVQADataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each sample corresponds to one image group:\n",
        "      - image_path\n",
        "      - questions (list)\n",
        "      - answers (list)\n",
        "      - organ (metadata)\n",
        "\n",
        "    Produces:\n",
        "      - fused: torch.Tensor [dim]\n",
        "      - target: torch.Tensor [num_classes] (multi-hot float32)\n",
        "\n",
        "    New:\n",
        "      - computes self.class_counts : torch.Tensor [num_classes] (counts of positive examples per class)\n",
        "      - exposes get_pos_weight() to compute the standard BCE pos_weight = (neg / pos)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 groups: List[Dict[str, Any]],\n",
        "                 label_map: Dict[str, int],\n",
        "                 pipeline: Optional[TreeCNNPipeline] = None,\n",
        "                 device: str = 'cpu',\n",
        "                 precompute: bool = False):\n",
        "        self.groups = groups\n",
        "        self.label_map = label_map\n",
        "        self.num_classes = len(label_map)\n",
        "        self.pipeline = pipeline\n",
        "        self.device = device\n",
        "        self.precompute = precompute\n",
        "\n",
        "        self.builder = GroupTreeBuilder(device=device, dim=50, max_workers=8)\n",
        "\n",
        "        # compute class counts from groups (positive counts)\n",
        "        self.class_counts = torch.zeros(self.num_classes, dtype=torch.long)\n",
        "        for g in self.groups:\n",
        "            answers = g.get(\"answers\", [])\n",
        "            if not answers:\n",
        "                continue\n",
        "            for a in answers:\n",
        "                if not a:\n",
        "                    continue\n",
        "                idx = self.label_map.get(a)\n",
        "                if idx is not None:\n",
        "                    self.class_counts[idx] += 1\n",
        "\n",
        "        # precompute fused vectors and multi-hot targets if requested\n",
        "        if self.precompute:\n",
        "            self.precomputed = []\n",
        "            for g in self.groups:\n",
        "                fused = self._compute_group_fused(g)\n",
        "                target = self._aggregate_target(g[\"answers\"])\n",
        "                # store on CPU; moved to device at retrieval time\n",
        "                self.precomputed.append({\n",
        "                    \"fused\": fused.detach().cpu(),\n",
        "                    \"target\": target.detach().cpu()\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.groups)\n",
        "\n",
        "    def _aggregate_target(self, answers: List[str]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Return multi-hot vector (float) of length num_classes.\n",
        "        If an answer is not in label_map it's ignored.\n",
        "        \"\"\"\n",
        "        target = torch.zeros(self.num_classes, dtype=torch.float32)\n",
        "        for a in answers:\n",
        "            if not a:\n",
        "                continue\n",
        "            idx = self.label_map.get(a)\n",
        "            if idx is not None:\n",
        "                target[idx] = 1.0\n",
        "        return target\n",
        "\n",
        "    def _compute_group_fused(self, group):\n",
        "        \"\"\"\n",
        "        Build combined tree and fuse via pipeline (if provided).\n",
        "        If pipeline is None or fusion fails we return zeros.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            combined = self.builder.build_combined_for_group(group[\"image_path\"], group[\"questions\"], organ=group.get(\"organ\"))\n",
        "            if self.pipeline is not None:\n",
        "                fused = self.pipeline.process_group(combined)\n",
        "                if fused is None:\n",
        "                    return torch.zeros(self.pipeline.dim, device=self.device)\n",
        "                return fused.to(self.device)\n",
        "            else:\n",
        "                # fallback zero vector\n",
        "                return torch.zeros(64, device=self.device)\n",
        "        except Exception:\n",
        "            return torch.zeros(self.pipeline.dim if self.pipeline is not None else 64, device=self.device)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.precompute:\n",
        "            ex = self.precomputed[idx]\n",
        "            return {\n",
        "                \"fused\": ex[\"fused\"].to(self.device),\n",
        "                \"target\": ex[\"target\"].to(self.device)\n",
        "            }\n",
        "        group = self.groups[idx]\n",
        "        fused = self._compute_group_fused(group)\n",
        "        target_idx = self._aggregate_target(group[\"answers\"])\n",
        "        return {\"fused\": fused.to(self.device), \"target\": target_idx.to(self.device)}\n",
        "\n",
        "    # ----------------- helper for Trainer -----------------\n",
        "    def get_pos_weight(self, eps: float = 1e-6) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Return pos_weight tensor for BCEWithLogitsLoss:\n",
        "          pos_weight[c] = (num_neg_examples / num_pos_examples) = ((N - pos_c) / pos_c)\n",
        "        Handles zero-count classes safely by capping.\n",
        "        Returns float tensor (on CPU); Trainer will move to device.\n",
        "        \"\"\"\n",
        "        N = len(self.groups)\n",
        "        pos = self.class_counts.float()\n",
        "        # avoid division by zero: if pos == 0, set pos to eps (small) -> very large weight\n",
        "        pos_safe = torch.where(pos <= 0.0, torch.full_like(pos, eps), pos)\n",
        "        neg = float(N) - pos\n",
        "        # pos_weight = neg / pos\n",
        "        pos_weight = neg / pos_safe\n",
        "        # clamp to avoid infinite/huge values (optional)\n",
        "        pos_weight = torch.clamp(pos_weight, min=1.0, max=10.0)\n",
        "        return pos_weight.float()\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    batch: list of dicts { 'fused': [dim], 'target': [num_classes] }\n",
        "    returns stacked tensors\n",
        "    \"\"\"\n",
        "    fused = torch.stack([b['fused'] for b in batch], dim=0)\n",
        "    targets = torch.stack([b['target'] for b in batch], dim=0)\n",
        "    return {'fused': fused, 'target': targets}\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Deep multi-label classification head\n",
        "# -------------------------\n",
        "class DeepHead(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes, hidden_dims=[256,128], dropout=0.3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev = in_dim\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(prev, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.LayerNorm(h))\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)  # [B, num_classes]\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Trainer (multi-label)\n",
        "# -------------------------\n",
        "class Trainer:\n",
        "    def __init__(self,\n",
        "                 model: nn.Module,\n",
        "                 optimizer,\n",
        "                 loss_fn: Optional[nn.Module] = None,\n",
        "                 device: str = 'cpu',\n",
        "                 ckpt_dir: str = './checkpoints',\n",
        "                 dataset_for_weights: Optional[GroupVQADataset] = None,\n",
        "                 use_pos_weight: bool = True):\n",
        "        \"\"\"\n",
        "        model: nn.Module\n",
        "        optimizer: optimizer\n",
        "        loss_fn: optional; if None we'll create BCEWithLogitsLoss (multi-label).\n",
        "                 If dataset_for_weights is provided and use_pos_weight=True we'll compute pos_weight automatically.\n",
        "        dataset_for_weights: pass the GroupVQADataset instance to compute pos-weight from its class_counts\n",
        "        \"\"\"\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.ckpt_dir = ckpt_dir\n",
        "        os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "        # Setup loss: prefer provided; else build BCEWithLogitsLoss with optional pos_weight\n",
        "        if loss_fn is not None:\n",
        "            self.loss_fn = loss_fn\n",
        "        else:\n",
        "            pos_weight = None\n",
        "            if dataset_for_weights is not None and use_pos_weight:\n",
        "                try:\n",
        "                    pos_weight = dataset_for_weights.get_pos_weight()\n",
        "                    # move to device\n",
        "                    pos_weight = pos_weight.to(device)\n",
        "                except Exception:\n",
        "                    pos_weight = None\n",
        "            if pos_weight is not None:\n",
        "                self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "            else:\n",
        "                self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def _compute_metrics(self, preds_logits: torch.Tensor, targets: torch.Tensor, threshold: float = 0.5):\n",
        "        \"\"\"\n",
        "        preds_logits: [N, C] (logits)\n",
        "        targets: [N, C] (0./1.)\n",
        "        Returns: dict with accuracy, precision, recall, f1 (macro)\n",
        "        Uses sklearn if available, else fallback to micro calculations.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            probs = torch.sigmoid(preds_logits).cpu()\n",
        "            preds = (probs >= threshold).long().numpy()\n",
        "            targs = targets.cpu().long().numpy()\n",
        "\n",
        "        metrics = {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
        "\n",
        "        try:\n",
        "            if _HAVE_SKLEARN:\n",
        "                # accuracy here is element-wise match fraction\n",
        "                acc = (preds == targs).mean()\n",
        "                prec = precision_score(targs, preds, average='macro', zero_division=0)\n",
        "                rec = recall_score(targs, preds, average='macro', zero_division=0)\n",
        "                f1 = f1_score(targs, preds, average='macro', zero_division=0)\n",
        "                metrics.update({\"accuracy\": float(acc), \"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1)})\n",
        "            else:\n",
        "                tp = int(((preds == 1) & (targs == 1)).sum())\n",
        "                fp = int(((preds == 1) & (targs == 0)).sum())\n",
        "                fn = int(((preds == 0) & (targs == 1)).sum())\n",
        "                tn = int(((preds == 0) & (targs == 0)).sum())\n",
        "                acc = float((tp + tn) / max(1, (tp + tn + fp + fn)))\n",
        "                prec = float(tp / max(1, (tp + fp)))\n",
        "                rec = float(tp / max(1, (tp + fn)))\n",
        "                f1 = float(2 * prec * rec / max(1e-8, prec + rec))\n",
        "                metrics.update({\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1})\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def train_epoch(self, dataloader: DataLoader, log_every: int = 10, threshold: float = 0.5):\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        count = 0\n",
        "        start = time.time()\n",
        "\n",
        "        all_logits = []\n",
        "        all_targets = []\n",
        "\n",
        "        num_steps = len(dataloader)\n",
        "        bar_len = 40\n",
        "        iter_start = time.time()\n",
        "\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            fused = batch['fused'].to(self.device)\n",
        "            targets = batch['target'].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            logits = self.model(fused)  # [B, C]\n",
        "            loss = self.loss_fn(logits, targets)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += float(loss.item())\n",
        "            count += 1\n",
        "\n",
        "            all_logits.append(logits.detach().cpu())\n",
        "            all_targets.append(targets.detach().cpu())\n",
        "\n",
        "            # Progress / ETA\n",
        "            progress = (i + 1) / num_steps\n",
        "            filled = int(progress * bar_len)\n",
        "            bar = \"█\" * filled + \"-\" * (bar_len - filled)\n",
        "            elapsed = time.time() - iter_start\n",
        "            eta = (elapsed / (i + 1)) * (num_steps - (i + 1)) if (i + 1) > 0 else 0.0\n",
        "\n",
        "            if (i + 1) % max(1, log_every) == 0 or (i + 1) == num_steps:\n",
        "                avg_loss = total_loss / max(1, count)\n",
        "                print(f\"\\r[{bar}] {progress*100:5.1f}% | Loss: {avg_loss:.4f} | ETA: {eta:5.1f}s\", end=\"\")\n",
        "\n",
        "        # End epoch newline\n",
        "        print()\n",
        "\n",
        "        # Aggregate metrics\n",
        "        # Aggregate metrics\n",
        "        logits_all = torch.cat(all_logits, dim=0)\n",
        "        targets_all = torch.cat(all_targets, dim=0)\n",
        "\n",
        "        # ---- ADAPTIVE THRESHOLD ----\n",
        "        best_thr, best_f1 = self.find_best_threshold(logits_all, targets_all)\n",
        "        print(f\"\\n🔧 Best threshold this epoch = {best_thr:.2f} (F1 = {best_f1:.4f})\")\n",
        "\n",
        "        # compute metrics using new threshold\n",
        "        metrics = self._compute_metrics(logits_all, targets_all, threshold=best_thr)\n",
        "\n",
        "        elapsed_total = time.time() - start\n",
        "        epoch_loss = total_loss / max(1, count)\n",
        "\n",
        "        return epoch_loss, elapsed_total, metrics\n",
        "\n",
        "    def evaluate(self, dataloader: DataLoader, threshold: float = 0.5):\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        count = 0\n",
        "        all_logits = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                fused = batch['fused'].to(self.device)\n",
        "                targets = batch['target'].to(self.device)\n",
        "\n",
        "                logits = self.model(fused)\n",
        "                loss = self.loss_fn(logits, targets)\n",
        "\n",
        "                total_loss += float(loss.item())\n",
        "                count += 1\n",
        "                all_logits.append(logits.cpu())\n",
        "                all_targets.append(targets.cpu())\n",
        "\n",
        "        if count == 0:\n",
        "            return 0.0, {}\n",
        "\n",
        "        logits_all = torch.cat(all_logits, dim=0)\n",
        "        targets_all = torch.cat(all_targets, dim=0)\n",
        "        metrics = self._compute_metrics(logits_all, targets_all, threshold=threshold)\n",
        "        return total_loss / count, metrics\n",
        "\n",
        "    def save(self, name='latest.pt'):\n",
        "        path = os.path.join(self.ckpt_dir, name)\n",
        "        torch.save({'model_state': self.model.state_dict()}, path)\n",
        "        return path\n",
        "\n",
        "    def load(self, path):\n",
        "        ck = torch.load(path, map_location=self.device)\n",
        "        self.model.load_state_dict(ck['model_state'])\n",
        "\n",
        "    def find_best_threshold(self, logits, targets):\n",
        "       thresholds = [i/100 for i in range(5, 80)]  # 0.05 → 0.95\n",
        "       best_thr = 0.5\n",
        "       best_f1 = 0\n",
        "\n",
        "       with torch.no_grad():\n",
        "           probs = torch.sigmoid(logits).cpu()\n",
        "           targs = targets.cpu().long().numpy()\n",
        "\n",
        "           for thr in thresholds:\n",
        "               preds = (probs >= thr).long().numpy()\n",
        "               f1 = f1_score(targs, preds, average='macro', zero_division=0)\n",
        "\n",
        "               if f1 > best_f1:\n",
        "                   best_f1 = f1\n",
        "                   best_thr = thr\n",
        "\n",
        "       return best_thr, best_f1\n",
        "# -------------------------\n",
        "# Example usage snippet (kept here for convenience)\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # small sanity example\n",
        "    csv_path = \"/content/vqa_rad.csv\"\n",
        "    loader = GroupedDatasetLoader(csv_path=csv_path)\n",
        "    groups, label_map = loader.load_and_group()\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    pipeline = TreeCNNPipeline(device=device, dim=64)\n",
        "\n",
        "    dataset = GroupVQADataset(groups, label_map, pipeline=pipeline, device=device, precompute=False)\n",
        "    dataloader = DataLoader(dataset, batch_size=4, collate_fn=collate_batch)\n",
        "\n",
        "    input_dim = pipeline.dim\n",
        "    model = DeepHead(in_dim=input_dim, num_classes=len(label_map)).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "    # Build Trainer that automatically computes pos_weight from dataset.class_counts\n",
        "    trainer = Trainer(model, optimizer, loss_fn=None, device=device, dataset_for_weights=dataset, use_pos_weight=True)\n",
        "\n",
        "    for epoch in range(2):\n",
        "        loss, elapsed, metrics = trainer.train_epoch(dataloader)\n",
        "        print(f\"Epoch {epoch} loss={loss:.4f} time={elapsed:.1f}s metrics={metrics}\")"
      ],
      "metadata": {
        "id": "lWUy51_quJsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127f9136-99c5-4e48-fc74-d83e53046c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/trainer_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_path = \"/content/VQA_RAD Image Folder.zip\"\n",
        "# Destination directory\n",
        "extract_path = \"/content/images\"\n",
        "\n",
        "# Make sure the destination directory exists\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to {extract_path}\")"
      ],
      "metadata": {
        "id": "YYl9Gs5mjI8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c75678-ee1f-4193-eb28-c0cb661fe6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import trainer_pipeline\n",
        "importlib.reload(trainer_pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFKbmKZx6vsF",
        "outputId": "4ddc04d0-8bc5-4ca3-8d22-ef681157bcab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'trainer_pipeline' from '/content/trainer_pipeline.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import importlib\n",
        "\n",
        "# Reload FIRST\n",
        "import trainer_pipeline\n",
        "importlib.reload(trainer_pipeline)\n",
        "\n",
        "from trainer_pipeline import Trainer, collate_batch, GroupVQADataset, DeepHead\n",
        "from dataset_group_loader import GroupedDatasetLoader\n",
        "from treecnn_pipeline import TreeCNNPipeline\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1. Load Dataset\n",
        "# ----------------------------------------\n",
        "loader_src = GroupedDatasetLoader(\n",
        "    csv_path=\"/content/VQA_RAD Dataset Public.xlsx\",\n",
        "    image_root=\"/content/images\"\n",
        ")\n",
        "\n",
        "groups, label_map = loader_src.load_and_group()\n",
        "num_classes = len(label_map)\n",
        "print(\"Number of classes =\", num_classes)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2. Pipeline + Dataset\n",
        "# ----------------------------------------\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "pipeline = TreeCNNPipeline(device=device, dim=64)\n",
        "\n",
        "dataset = GroupVQADataset(\n",
        "    groups,\n",
        "    label_map,\n",
        "    pipeline=pipeline,\n",
        "    device=device,\n",
        "    precompute=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=8,\n",
        "    collate_fn=collate_batch,\n",
        "    num_workers=2,\n",
        "    pin_memory=(device=='cuda')\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3. Deep Multi-Label Head\n",
        "# ----------------------------------------\n",
        "model = DeepHead(\n",
        "    in_dim=64,\n",
        "    num_classes=num_classes\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Trainer automatically builds pos_weight + BCE loss\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=None,                 # ← Let trainer build BCEWithLogits + pos_weight\n",
        "    device=device,\n",
        "    dataset_for_weights=dataset,  # ← Required for pos_weight\n",
        "    use_pos_weight=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4. TRAIN (NOW USING ADAPTIVE THRESHOLD)\n",
        "# ----------------------------------------\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # NOTE: DO NOT pass threshold anymore — adaptive threshold is inside Trainer\n",
        "    loss, elapsed, metrics = trainer.train_epoch(train_loader)\n",
        "\n",
        "    print(f\"🟩 Epoch {epoch+1}/{EPOCHS} Summary:\")\n",
        "    print(f\"   Loss      = {loss:.4f}\")\n",
        "    print(f\"   Accuracy  = {metrics['accuracy']:.4f}\")\n",
        "    print(f\"   Precision = {metrics['precision']:.4f}\")\n",
        "    print(f\"   Recall    = {metrics['recall']:.4f}\")\n",
        "    print(f\"   F1 Score  = {metrics['f1']:.4f}\")\n",
        "    print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "id": "hs-4gX9sc-3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c9550e-04ef-47c5-d94f-a8cd75635d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ Total unique answers after normalization = 7\n",
            "Number of classes = 7\n",
            "[████████████████████████████████████████] 100.0% | Loss: 0.5836 | ETA:   0.0s\n",
            "\n",
            "🔧 Best threshold this epoch = 0.05 (F1 = 0.3981)\n",
            "🟩 Epoch 1/10 Summary:\n",
            "   Loss      = 0.5836\n",
            "   Accuracy  = 0.4445\n",
            "   Precision = 0.3389\n",
            "   Recall    = 0.8333\n",
            "   F1 Score  = 0.3981\n",
            "--------------------------------------------------\n",
            "[████████████████████████████████████████] 100.0% | Loss: 0.5325 | ETA:   0.0s\n",
            "\n",
            "🔧 Best threshold this epoch = 0.11 (F1 = 0.4013)\n",
            "🟩 Epoch 2/10 Summary:\n",
            "   Loss      = 0.5325\n",
            "   Accuracy  = 0.6192\n",
            "   Precision = 0.3406\n",
            "   Recall    = 0.7381\n",
            "   F1 Score  = 0.4013\n",
            "--------------------------------------------------\n",
            "[████████████████████████████████████████] 100.0% | Loss: 0.5363 | ETA:   0.0s\n",
            "\n",
            "🔧 Best threshold this epoch = 0.15 (F1 = 0.4526)\n",
            "🟩 Epoch 3/10 Summary:\n",
            "   Loss      = 0.5363\n",
            "   Accuracy  = 0.6738\n",
            "   Precision = 0.3853\n",
            "   Recall    = 0.7143\n",
            "   F1 Score  = 0.4526\n",
            "--------------------------------------------------\n",
            "[████████████████████████████████████████] 100.0% | Loss: 0.5438 | ETA:   0.0s\n",
            "\n",
            "🔧 Best threshold this epoch = 0.05 (F1 = 0.4011)\n",
            "🟩 Epoch 4/10 Summary:\n",
            "   Loss      = 0.5438\n",
            "   Accuracy  = 0.4167\n",
            "   Precision = 0.3404\n",
            "   Recall    = 0.9286\n",
            "   F1 Score  = 0.4011\n",
            "--------------------------------------------------\n",
            "[████████████████████████████████████████] 100.0% | Loss: 0.5244 | ETA:   0.0s\n",
            "\n",
            "🔧 Best threshold this epoch = 0.12 (F1 = 0.4115)\n",
            "🟩 Epoch 5/10 Summary:\n",
            "   Loss      = 0.5244\n",
            "   Accuracy  = 0.6019\n",
            "   Precision = 0.3465\n",
            "   Recall    = 0.7619\n",
            "   F1 Score  = 0.4115\n",
            "--------------------------------------------------\n",
            "[████████████████████████████████████████] 100.0% | Loss: 0.5437 | ETA:   0.0s\n",
            "\n",
            "🔧 Best threshold this epoch = 0.06 (F1 = 0.3993)\n",
            "🟩 Epoch 6/10 Summary:\n",
            "   Loss      = 0.5437\n",
            "   Accuracy  = 0.4227\n",
            "   Precision = 0.3395\n",
            "   Recall    = 0.8571\n",
            "   F1 Score  = 0.3993\n",
            "--------------------------------------------------\n",
            "[████████████████████████████████████████] 100.0% | Loss: 0.5391 | ETA:   0.0s\n",
            "\n",
            "🔧 Best threshold this epoch = 0.08 (F1 = 0.4025)\n",
            "🟩 Epoch 7/10 Summary:\n",
            "   Loss      = 0.5391\n",
            "   Accuracy  = 0.4768\n",
            "   Precision = 0.3412\n",
            "   Recall    = 0.8929\n",
            "   F1 Score  = 0.4025\n",
            "--------------------------------------------------\n",
            "[████████████████████████████████████████] 100.0% | Loss: 0.5378 | ETA:   0.0s\n",
            "\n",
            "🔧 Best threshold this epoch = 0.05 (F1 = 0.4006)\n",
            "🟩 Epoch 8/10 Summary:\n",
            "   Loss      = 0.5378\n",
            "   Accuracy  = 0.3954\n",
            "   Precision = 0.3402\n",
            "   Recall    = 0.9286\n",
            "   F1 Score  = 0.4006\n",
            "--------------------------------------------------\n",
            "[████████████████████████████████████████] 100.0% | Loss: 0.5277 | ETA:   0.0s\n",
            "\n",
            "🔧 Best threshold this epoch = 0.08 (F1 = 0.4039)\n",
            "🟩 Epoch 9/10 Summary:\n",
            "   Loss      = 0.5277\n",
            "   Accuracy  = 0.4750\n",
            "   Precision = 0.3419\n",
            "   Recall    = 0.9286\n",
            "   F1 Score  = 0.4039\n",
            "--------------------------------------------------\n",
            "[████████████████████████████████████████] 100.0% | Loss: 0.5134 | ETA:   0.0s\n",
            "\n",
            "🔧 Best threshold this epoch = 0.09 (F1 = 0.4095)\n",
            "🟩 Epoch 10/10 Summary:\n",
            "   Loss      = 0.5134\n",
            "   Accuracy  = 0.5177\n",
            "   Precision = 0.3449\n",
            "   Recall    = 0.9286\n",
            "   F1 Score  = 0.4095\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}